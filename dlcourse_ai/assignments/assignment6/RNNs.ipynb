{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "#!pip3 -qq install bokeh==0.13.0\n",
    "#!pip3 -qq install gensim==3.6.0\n",
    "#!pip3 -qq install nltk\n",
    "#!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    print(True)\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    print(False)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\N1keFan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\N1keFan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'PRT', 'VERB', 'ADV', 'ADJ', 'ADP', '.', 'NOUN', 'CONJ', 'X', 'DET', 'PRON', 'NUM'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # <create layers>\n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=False)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <apply them>\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0761\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits, 2)\n",
    "current_count = ((preds == y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.58\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#<calc loss>\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "print('Loss : {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                #<calc loss>\n",
    "                loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # <calc accuracy>\n",
    "                mask = (y_batch != 0).float()\n",
    "                preds = torch.argmax(logits, 2)\n",
    "                cur_correct_count = ((preds == y_batch).float() * mask).sum()\n",
    "                cur_sum_count = mask.sum()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.31559, Accuracy = 71.89%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 49.30it/s]\n",
      "[1 / 20]   Val: Loss = 0.10451, Accuracy = 85.09%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 45.58it/s]\n",
      "[2 / 20] Train: Loss = 0.10041, Accuracy = 90.01%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 50.64it/s]\n",
      "[2 / 20]   Val: Loss = 0.07180, Accuracy = 89.61%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 45.53it/s]\n",
      "[3 / 20] Train: Loss = 0.06746, Accuracy = 93.25%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 50.52it/s]\n",
      "[3 / 20]   Val: Loss = 0.06408, Accuracy = 91.30%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 43.74it/s]\n",
      "[4 / 20] Train: Loss = 0.05045, Accuracy = 94.86%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 51.26it/s]\n",
      "[4 / 20]   Val: Loss = 0.06326, Accuracy = 92.01%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 45.26it/s]\n",
      "[5 / 20] Train: Loss = 0.04039, Accuracy = 95.86%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 51.28it/s]\n",
      "[5 / 20]   Val: Loss = 0.06164, Accuracy = 92.60%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 44.26it/s]\n",
      "[6 / 20] Train: Loss = 0.03282, Accuracy = 96.61%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 50.45it/s]\n",
      "[6 / 20]   Val: Loss = 0.05890, Accuracy = 92.94%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 42.69it/s]\n",
      "[7 / 20] Train: Loss = 0.02723, Accuracy = 97.17%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 49.83it/s]\n",
      "[7 / 20]   Val: Loss = 0.06006, Accuracy = 93.07%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 42.88it/s]\n",
      "[8 / 20] Train: Loss = 0.02236, Accuracy = 97.69%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 49.68it/s]\n",
      "[8 / 20]   Val: Loss = 0.06161, Accuracy = 93.18%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 41.25it/s]\n",
      "[9 / 20] Train: Loss = 0.01850, Accuracy = 98.08%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 50.37it/s]\n",
      "[9 / 20]   Val: Loss = 0.06880, Accuracy = 93.20%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 42.46it/s]\n",
      "[10 / 20] Train: Loss = 0.01541, Accuracy = 98.40%: 100%|████████████████████████████| 572/572 [00:11<00:00, 48.61it/s]\n",
      "[10 / 20]   Val: Loss = 0.07278, Accuracy = 93.20%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 42.77it/s]\n",
      "[11 / 20] Train: Loss = 0.01269, Accuracy = 98.71%: 100%|████████████████████████████| 572/572 [00:11<00:00, 48.94it/s]\n",
      "[11 / 20]   Val: Loss = 0.06622, Accuracy = 93.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 38.45it/s]\n",
      "[12 / 20] Train: Loss = 0.01048, Accuracy = 98.96%: 100%|████████████████████████████| 572/572 [00:12<00:00, 45.40it/s]\n",
      "[12 / 20]   Val: Loss = 0.07409, Accuracy = 93.21%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 38.91it/s]\n",
      "[13 / 20] Train: Loss = 0.00843, Accuracy = 99.17%: 100%|████████████████████████████| 572/572 [00:12<00:00, 46.09it/s]\n",
      "[13 / 20]   Val: Loss = 0.07219, Accuracy = 93.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 39.03it/s]\n",
      "[14 / 20] Train: Loss = 0.00682, Accuracy = 99.34%: 100%|████████████████████████████| 572/572 [00:12<00:00, 46.93it/s]\n",
      "[14 / 20]   Val: Loss = 0.07365, Accuracy = 93.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 37.46it/s]\n",
      "[15 / 20] Train: Loss = 0.00556, Accuracy = 99.48%: 100%|████████████████████████████| 572/572 [00:12<00:00, 45.56it/s]\n",
      "[15 / 20]   Val: Loss = 0.08413, Accuracy = 93.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 36.67it/s]\n",
      "[16 / 20] Train: Loss = 0.00443, Accuracy = 99.60%: 100%|████████████████████████████| 572/572 [00:13<00:00, 43.93it/s]\n",
      "[16 / 20]   Val: Loss = 0.08402, Accuracy = 93.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 33.75it/s]\n",
      "[17 / 20] Train: Loss = 0.00366, Accuracy = 99.67%: 100%|████████████████████████████| 572/572 [00:13<00:00, 45.81it/s]\n",
      "[17 / 20]   Val: Loss = 0.08828, Accuracy = 93.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.32it/s]\n",
      "[18 / 20] Train: Loss = 0.00308, Accuracy = 99.72%: 100%|████████████████████████████| 572/572 [00:13<00:00, 43.35it/s]\n",
      "[18 / 20]   Val: Loss = 0.10034, Accuracy = 92.97%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 38.01it/s]\n",
      "[19 / 20] Train: Loss = 0.00256, Accuracy = 99.77%: 100%|████████████████████████████| 572/572 [00:13<00:00, 42.85it/s]\n",
      "[19 / 20]   Val: Loss = 0.09633, Accuracy = 92.99%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 37.09it/s]\n",
      "[20 / 20] Train: Loss = 0.00233, Accuracy = 99.79%: 100%|████████████████████████████| 572/572 [00:13<00:00, 43.97it/s]\n",
      "[20 / 20]   Val: Loss = 0.10486, Accuracy = 92.94%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 35.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0761\n",
      "Loss : 2.57\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits, 2)\n",
    "current_count = ((preds == y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))\n",
    "\n",
    "#<calc loss>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "print('Loss : {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # <create layers>\n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <apply them>\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.55695, Accuracy = 82.48%: 100%|█████████████████████████████| 572/572 [00:13<00:00, 42.12it/s]\n",
      "[1 / 20]   Val: Loss = 0.27744, Accuracy = 90.93%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.92it/s]\n",
      "[2 / 20] Train: Loss = 0.20635, Accuracy = 93.42%: 100%|█████████████████████████████| 572/572 [00:13<00:00, 41.70it/s]\n",
      "[2 / 20]   Val: Loss = 0.18061, Accuracy = 94.16%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 29.86it/s]\n",
      "[3 / 20] Train: Loss = 0.13030, Accuracy = 95.98%: 100%|█████████████████████████████| 572/572 [00:14<00:00, 40.68it/s]\n",
      "[3 / 20]   Val: Loss = 0.14544, Accuracy = 95.20%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.34it/s]\n",
      "[4 / 20] Train: Loss = 0.08905, Accuracy = 97.29%: 100%|█████████████████████████████| 572/572 [00:14<00:00, 40.48it/s]\n",
      "[4 / 20]   Val: Loss = 0.12750, Accuracy = 95.78%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.28it/s]\n",
      "[5 / 20] Train: Loss = 0.06139, Accuracy = 98.16%: 100%|█████████████████████████████| 572/572 [00:15<00:00, 32.90it/s]\n",
      "[5 / 20]   Val: Loss = 0.12659, Accuracy = 95.87%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.33it/s]\n",
      "[6 / 20] Train: Loss = 0.04190, Accuracy = 98.79%: 100%|█████████████████████████████| 572/572 [00:15<00:00, 38.33it/s]\n",
      "[6 / 20]   Val: Loss = 0.11683, Accuracy = 96.22%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.97it/s]\n",
      "[7 / 20] Train: Loss = 0.02791, Accuracy = 99.25%: 100%|█████████████████████████████| 572/572 [00:16<00:00, 35.45it/s]\n",
      "[7 / 20]   Val: Loss = 0.13185, Accuracy = 96.01%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 24.92it/s]\n",
      "[8 / 20] Train: Loss = 0.01817, Accuracy = 99.54%: 100%|█████████████████████████████| 572/572 [00:17<00:00, 32.86it/s]\n",
      "[8 / 20]   Val: Loss = 0.13592, Accuracy = 96.06%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 22.95it/s]\n",
      "[9 / 20] Train: Loss = 0.01156, Accuracy = 99.74%: 100%|█████████████████████████████| 572/572 [00:19<00:00, 29.59it/s]\n",
      "[9 / 20]   Val: Loss = 0.12918, Accuracy = 96.34%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 24.92it/s]\n",
      "[10 / 20] Train: Loss = 0.00714, Accuracy = 99.86%: 100%|████████████████████████████| 572/572 [00:16<00:00, 34.91it/s]\n",
      "[10 / 20]   Val: Loss = 0.14668, Accuracy = 96.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 26.31it/s]\n",
      "[11 / 20] Train: Loss = 0.00448, Accuracy = 99.93%: 100%|████████████████████████████| 572/572 [00:16<00:00, 35.26it/s]\n",
      "[11 / 20]   Val: Loss = 0.15660, Accuracy = 96.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.59it/s]\n",
      "[12 / 20] Train: Loss = 0.00275, Accuracy = 99.97%: 100%|████████████████████████████| 572/572 [00:16<00:00, 35.00it/s]\n",
      "[12 / 20]   Val: Loss = 0.15780, Accuracy = 96.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.63it/s]\n",
      "[13 / 20] Train: Loss = 0.00179, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:16<00:00, 34.31it/s]\n",
      "[13 / 20]   Val: Loss = 0.17081, Accuracy = 96.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.63it/s]\n",
      "[14 / 20] Train: Loss = 0.00127, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:16<00:00, 33.79it/s]\n",
      "[14 / 20]   Val: Loss = 0.18360, Accuracy = 96.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.46it/s]\n",
      "[15 / 20] Train: Loss = 0.00092, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:17<00:00, 33.23it/s]\n",
      "[15 / 20]   Val: Loss = 0.18302, Accuracy = 96.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 24.39it/s]\n",
      "[16 / 20] Train: Loss = 0.00286, Accuracy = 99.92%: 100%|████████████████████████████| 572/572 [00:17<00:00, 32.99it/s]\n",
      "[16 / 20]   Val: Loss = 0.18302, Accuracy = 96.00%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.66it/s]\n",
      "[17 / 20] Train: Loss = 0.00373, Accuracy = 99.90%: 100%|████████████████████████████| 572/572 [00:17<00:00, 32.58it/s]\n",
      "[17 / 20]   Val: Loss = 0.18045, Accuracy = 96.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 22.97it/s]\n",
      "[18 / 20] Train: Loss = 0.00078, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:17<00:00, 31.83it/s]\n",
      "[18 / 20]   Val: Loss = 0.18790, Accuracy = 96.17%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 24.23it/s]\n",
      "[19 / 20] Train: Loss = 0.00032, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:19<00:00, 29.84it/s]\n",
      "[19 / 20]   Val: Loss = 0.18935, Accuracy = 96.25%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 26.77it/s]\n",
      "[20 / 20] Train: Loss = 0.00022, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:18<00:00, 33.27it/s]\n",
      "[20 / 20]   Val: Loss = 0.19157, Accuracy = 96.26%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 25.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.141\n",
      "Loss : 2.56\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits, 2)\n",
    "current_count = ((preds == y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))\n",
    "\n",
    "#<calc loss>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "print('Loss : {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # <create me>\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count, bidirectional=False)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <use me>\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.75390, Accuracy = 78.12%: 100%|█████████████████████████████| 572/572 [00:11<00:00, 51.32it/s]\n",
      "[1 / 20]   Val: Loss = 0.37022, Accuracy = 89.19%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 53.09it/s]\n",
      "[2 / 20] Train: Loss = 0.28543, Accuracy = 91.39%: 100%|█████████████████████████████| 572/572 [00:08<00:00, 70.74it/s]\n",
      "[2 / 20]   Val: Loss = 0.25979, Accuracy = 91.97%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 56.92it/s]\n",
      "[3 / 20] Train: Loss = 0.20995, Accuracy = 93.44%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 71.77it/s]\n",
      "[3 / 20]   Val: Loss = 0.21149, Accuracy = 93.35%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 59.87it/s]\n",
      "[4 / 20] Train: Loss = 0.17320, Accuracy = 94.47%: 100%|█████████████████████████████| 572/572 [00:08<00:00, 70.70it/s]\n",
      "[4 / 20]   Val: Loss = 0.18478, Accuracy = 94.01%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 60.61it/s]\n",
      "[5 / 20] Train: Loss = 0.15185, Accuracy = 95.05%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 70.71it/s]\n",
      "[5 / 20]   Val: Loss = 0.16920, Accuracy = 94.45%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 62.98it/s]\n",
      "[6 / 20] Train: Loss = 0.13790, Accuracy = 95.46%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 72.61it/s]\n",
      "[6 / 20]   Val: Loss = 0.15997, Accuracy = 94.67%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.90it/s]\n",
      "[7 / 20] Train: Loss = 0.12807, Accuracy = 95.73%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 72.71it/s]\n",
      "[7 / 20]   Val: Loss = 0.15378, Accuracy = 94.89%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.74it/s]\n",
      "[8 / 20] Train: Loss = 0.12076, Accuracy = 95.93%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 73.99it/s]\n",
      "[8 / 20]   Val: Loss = 0.14908, Accuracy = 94.97%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 57.95it/s]\n",
      "[9 / 20] Train: Loss = 0.11525, Accuracy = 96.08%: 100%|█████████████████████████████| 572/572 [00:07<00:00, 72.46it/s]\n",
      "[9 / 20]   Val: Loss = 0.14502, Accuracy = 95.11%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 62.07it/s]\n",
      "[10 / 20] Train: Loss = 0.11064, Accuracy = 96.21%: 100%|████████████████████████████| 572/572 [00:07<00:00, 73.08it/s]\n",
      "[10 / 20]   Val: Loss = 0.14084, Accuracy = 95.30%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 64.53it/s]\n",
      "[11 / 20] Train: Loss = 0.10671, Accuracy = 96.32%: 100%|████████████████████████████| 572/572 [00:07<00:00, 72.29it/s]\n",
      "[11 / 20]   Val: Loss = 0.13906, Accuracy = 95.29%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 55.93it/s]\n",
      "[12 / 20] Train: Loss = 0.10347, Accuracy = 96.42%: 100%|████████████████████████████| 572/572 [00:07<00:00, 72.86it/s]\n",
      "[12 / 20]   Val: Loss = 0.13758, Accuracy = 95.30%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.77it/s]\n",
      "[13 / 20] Train: Loss = 0.10058, Accuracy = 96.49%: 100%|████████████████████████████| 572/572 [00:07<00:00, 72.69it/s]\n",
      "[13 / 20]   Val: Loss = 0.13923, Accuracy = 95.25%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.23it/s]\n",
      "[14 / 20] Train: Loss = 0.09817, Accuracy = 96.57%: 100%|████████████████████████████| 572/572 [00:07<00:00, 72.72it/s]\n",
      "[14 / 20]   Val: Loss = 0.13763, Accuracy = 95.38%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 54.65it/s]\n",
      "[15 / 20] Train: Loss = 0.09592, Accuracy = 96.65%: 100%|████████████████████████████| 572/572 [00:08<00:00, 70.48it/s]\n",
      "[15 / 20]   Val: Loss = 0.13513, Accuracy = 95.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 60.07it/s]\n",
      "[16 / 20] Train: Loss = 0.09386, Accuracy = 96.71%: 100%|████████████████████████████| 572/572 [00:08<00:00, 69.62it/s]\n",
      "[16 / 20]   Val: Loss = 0.13433, Accuracy = 95.46%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 60.22it/s]\n",
      "[17 / 20] Train: Loss = 0.09187, Accuracy = 96.76%: 100%|████████████████████████████| 572/572 [00:08<00:00, 70.77it/s]\n",
      "[17 / 20]   Val: Loss = 0.13417, Accuracy = 95.36%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 57.17it/s]\n",
      "[18 / 20] Train: Loss = 0.09016, Accuracy = 96.82%: 100%|████████████████████████████| 572/572 [00:08<00:00, 69.59it/s]\n",
      "[18 / 20]   Val: Loss = 0.13228, Accuracy = 95.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 60.02it/s]\n",
      "[19 / 20] Train: Loss = 0.08866, Accuracy = 96.86%: 100%|████████████████████████████| 572/572 [00:08<00:00, 68.94it/s]\n",
      "[19 / 20]   Val: Loss = 0.13298, Accuracy = 95.36%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.96it/s]\n",
      "[20 / 20] Train: Loss = 0.08705, Accuracy = 96.92%: 100%|████████████████████████████| 572/572 [00:08<00:00, 68.53it/s]\n",
      "[20 / 20]   Val: Loss = 0.13285, Accuracy = 95.45%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 56.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=torch.FloatTensor(embeddings),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTaggerWithPretrainedEmbs(\n",
       "  (emb): Embedding(45441, 100)\n",
       "  (lstm): LSTM(100, 64)\n",
       "  (linear): Linear(in_features=64, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0326\n",
      "Loss : 2.61\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=torch.FloatTensor(embeddings),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits, 2)\n",
    "current_count = ((preds == y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))\n",
    "\n",
    "#<calc loss>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "print('Loss : {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Variable[CUDAType] (got torch.FloatTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-245c5e6af97c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_embs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Variable[CUDAType] (got torch.FloatTensor)"
     ]
    }
   ],
   "source": [
    "# <calc test accuracy>\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    sentence_embs = []\n",
    "    sentence_tags = []\n",
    "    \n",
    "    for word, tag in sentence:\n",
    "        try:\n",
    "            word_emb = torch.from_numpy(w2v_model.get_vector(word.lower())).float()\n",
    "        except:\n",
    "            word_emb = torch.zeros(w2v_model.vectors.shape[1]).float()\n",
    "        sentence_embs.append(word_emb)\n",
    "        sentence_tags.append(tag2ind[tag])\n",
    "        \n",
    "    embeddings = torch.cat(sentence_embs, 0)\n",
    "    model = model.type(FloatTensor)\n",
    "    logits = model(FloatTensor(embeddings))\n",
    "    preds = torch.argmax(logits, 2)\n",
    "    correct += (preds == LongTensor(sentence_tags)).float().sum()\n",
    "    total += len(tags)\n",
    "    \n",
    "\n",
    "accuracy = correct / total\n",
    "print('Accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
