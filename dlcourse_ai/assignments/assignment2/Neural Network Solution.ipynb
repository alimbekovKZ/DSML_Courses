{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.79719785676 [[ 1.61599406  0.          2.76252524]\n",
      " [-0.         -0.5060347  -0.94302519]]\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут хранить через специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00418379929801 [[  4.18948095e-04   1.76337767e-03  -3.49979411e-04]\n",
      " [  7.82747412e-05  -6.12293143e-04  -1.09946675e-03]]\n",
      "Gradient check passed!\n",
      "0.020193253954 [[-1.67429365  2.15318718 -0.15893916 -0.56958173]\n",
      " [ 3.34858731 -4.30637437  0.31787831  1.13916345]\n",
      " [-0.36258122  5.45842018 -3.37773459 -0.89339967]]\n",
      "Gradient check passed!\n",
      "-0.0108456847394 [[-0.632844   -0.39002832  1.75340995  0.51081366]]\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08975373 -0.03568431  0.08994824 ..., -0.12654745 -0.03998549\n",
      "   0.09710392]\n",
      " [ 0.01612863  0.01921765  0.05465412 ..., -0.15007686 -0.13410314\n",
      "  -0.10681765]]\n",
      "Checking gradient for W-1\n",
      "2.30062671713 [[  1.05393580e-05   0.00000000e+00  -2.23528083e-05]\n",
      " [  1.25578983e-05   0.00000000e+00  -4.99938161e-06]\n",
      " [  3.57140937e-05   0.00000000e+00   4.39014438e-05]\n",
      " ..., \n",
      " [ -9.80687160e-05   0.00000000e+00  -8.39833186e-05]\n",
      " [ -8.76305797e-05   0.00000000e+00  -5.28532904e-05]\n",
      " [ -6.98006962e-05   0.00000000e+00  -2.94876473e-06]]\n",
      "Gradient check passed!\n",
      "Checking gradient for B-1\n",
      "2.30062671716 [[ 0.00065346  0.          0.0006072 ]]\n",
      "Gradient check passed!\n",
      "Checking gradient for W-2\n",
      "2.30062671109 [[ 0.00037503  0.00037535  0.00037567  0.0003749   0.0003757   0.00037455\n",
      "   0.00037548  0.00037493  0.00037505 -0.00337666]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.00050123  0.00050165  0.00050209  0.00050104  0.00050212  0.00050057\n",
      "   0.00050182  0.00050108  0.00050126 -0.00451284]]\n",
      "Gradient check passed!\n",
      "Checking gradient for B-2\n",
      "2.30062675622 [[ 0.09993815  0.10002254  0.10010892  0.09990119  0.10011637  0.09980764\n",
      "   0.1000566   0.09990923  0.09994333 -0.89980397]]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "print(train_X[:2])\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W-1\n",
      "2.30300066005 [[ -1.97632526e-04  -6.47184905e-05  -2.32640885e-04]\n",
      " [  1.53837763e-04   6.05300258e-05   8.69655510e-05]\n",
      " [  2.26173872e-04  -1.39763073e-05   1.49505283e-04]\n",
      " ..., \n",
      " [ -9.58146888e-05  -6.82463445e-06   1.58581489e-04]\n",
      " [  3.09991371e-04  -9.50381061e-05   2.49737747e-04]\n",
      " [  2.17393302e-04   1.89034051e-04   2.36248895e-04]]\n",
      "Gradient check passed!\n",
      "Checking gradient for B-1\n",
      "2.3030006577 [[-0.00010136  0.00061913 -0.00110427]]\n",
      "Gradient check passed!\n",
      "Checking gradient for W-2\n",
      "2.30300066875 [[  3.39012966e-05   2.96367547e-04  -3.31575631e-05   6.68423849e-06\n",
      "    5.16626623e-04   1.66051164e-04  -1.93627095e-04   9.95912794e-05\n",
      "    1.58614874e-04  -1.60238854e-04]\n",
      " [  7.86132071e-04   6.09759537e-04   9.07615761e-04   5.22062608e-04\n",
      "    6.89264761e-04   5.71071104e-04   9.13211773e-04   7.78545524e-04\n",
      "    9.27546318e-04  -7.99319194e-03]\n",
      " [  3.87328798e-05   3.53350809e-04  -3.44278575e-05   3.99531257e-04\n",
      "    1.64681499e-04   2.79508964e-05   4.06444350e-05   2.37841275e-04\n",
      "    6.34355760e-04  -1.21346801e-03]]\n",
      "Gradient check passed!\n",
      "Checking gradient for B-2\n",
      "2.3030006809 [[ 0.09960689  0.10020878  0.10050797  0.09963312  0.09981972  0.10009367\n",
      "   0.10002019  0.0995923   0.10023293 -0.89986285]]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e-1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  out of  30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.327327, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.316935, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.308675, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.302105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.296871, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.292700, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.289365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.286691, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.284543, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.282812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.281415, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.280287, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.279373, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.278630, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.278022, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.277527, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.277120, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.276787, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.276511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "1770  out of  9000\n",
      "206  out of  1000\n",
      "Loss: 2.276284, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f03199461705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# You should expect loss to go down and train and val accuracy go up for every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-57c8c0aa7e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-60b8d5323123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, learning_rate_decay=0.97)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history, learning_rate_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywithmaxval(d):\n",
    "     \"\"\" a) create a list of the dict's keys and values; \n",
    "         b) return the key with the max value\"\"\"  \n",
    "     v=list(d.values())\n",
    "     k=list(d.keys())\n",
    "     return k[v.index(max(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "reg_strengths = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "l_r_to_accuracy = {}\n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "\n",
    "        model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 110, reg = r)\n",
    "        # TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "        dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "        trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=l, learning_rate_decay=0.97, num_epochs=20, batch_size=5)\n",
    "        loss_history, train_history, val_history, learning_rate_history = trainer.fit()\n",
    "        l_r_to_accuracy[str(l) + ', ' + str(r)] = np.average(train_history)\n",
    "        print(\"Neural Network with l = %e, r = %e\" % (l, r))\n",
    "        print(\"Accuracy on training set: %4.2f\" % (l_r_to_accuracy[str(l) + ', ' + str(r)])) \n",
    "        #plt.plot(train_history)\n",
    "        #plt.plot(val_history)\n",
    "\n",
    "key_of_best = keywithmaxval(l_r_to_accuracy)\n",
    "print(keywithmaxval(l_r_to_accuracy), ' accuracy is ', (l_r_to_accuracy[key_of_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "learning_rate_history = []\n",
    "#reg = 0.01\n",
    "#learning_rate=0.1\n",
    "\n",
    "'''\n",
    "BEST OF THE BEST RECOMMENDED FOR EVERY DATA SCIENTIST!!!\n",
    "best_classifier = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 110, reg = 1e-6)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "best_trainer = Trainer(best_classifier, dataset, MomentumSGD(), learning_rate=1e-1, learning_rate_decay=0.97, num_epochs=50, batch_size=64)\n",
    "loss_history, train_history, val_history, learning_rate_history = best_trainer.fit()\n",
    "'''\n",
    "\n",
    "best_classifier = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 110, reg = 1e-6)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "best_trainer = Trainer(best_classifier, dataset, MomentumSGD(), learning_rate=1e-1, learning_rate_decay=0.97, num_epochs=50, batch_size=64)\n",
    "loss_history, train_history, val_history, learning_rate_history = best_trainer.fit()\n",
    "\n",
    "best_val_accuracy = np.average(val_history)\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Learning rate progress\")\n",
    "plt.plot(learning_rate_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
