{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO\n",
    "ELMO stands for Embeddings from Language Models. For details checkout the paper from Allen Institute https://arxiv.org/pdf/1802.05365.pdf\n",
    "\n",
    "The main idea behind the ELMO is to get contextualized word embeddings. In contrast to regular word2vec, which uses bag-of-words (order of words doesn't matter in this case) it builds dense represetations of the words using recurrent neural network that highly sensitive to the order of words.\n",
    "\n",
    "To make the representations of the words meaningful the network is optimised to solve language modelling task. The task of the Language Model is to predict next word given all previous words. The loss function for this task is the following:\n",
    "\n",
    "$L_i(x_i) = -log(p(x_i | x_0, .. , x_{i-1}))$\n",
    "\n",
    "For the backward direction and inversed word order the following formula is used:\n",
    "\n",
    "$L_i(x_i) = -log(p(x_i | x_{i+1}, .. , x_{N}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "We will use the Text8 dataset. This dataset consists of lowercased sequence of words from Wikipedia. All punctuation is replaced with whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100M/100M [00:01<00:00, 75.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "download_link = 'http://lnsigo.mipt.ru/export/datasets/text8'\n",
    "\n",
    "def download(file_name, source_url):\n",
    "    CHUNK = 16 * 1024\n",
    "\n",
    "    r = requests.get(source_url, stream=True)\n",
    "    total_length = int(r.headers.get('content-length', 0))\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pbar = tqdm(total=total_length, unit='B', unit_scale=True)\n",
    "        for chunk in r.iter_content(chunk_size=CHUNK):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                pbar.update(len(chunk))\n",
    "                f.write(chunk)\n",
    "        f.close()\n",
    "        \n",
    "download('text8', download_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset and split it into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dataset: 17005207\n"
     ]
    }
   ],
   "source": [
    "with open('text8') as f:\n",
    "    words = f.read().split()\n",
    "n_words = len(words)\n",
    "print('Number of words in the dataset: {}'.format(n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train validation and test sets\n",
    "Split the data to train validation and test sets with parts 0.8, 0.1 0.1 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = 0.8\n",
    "validation_part = 0.1\n",
    "test_part = 0.1\n",
    "n_train = int(n_words * train_part)\n",
    "n_valid = int(n_words * validation_part)\n",
    "n_test = n_words - n_train - n_valid\n",
    "train_set = words[:n_train]\n",
    "valid_set = words[n_train: n_train + n_valid]\n",
    "test_set = words[n_train + n_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Now we will create the vocabulary to perform conversion from tokens to indices and vice versa. This structure is needed to perform embedding lookup. We have a matrix with embeddings. Each row of this matrix corresponds to embedding of certain word. For example the row that corresponds to the word 'scientist' has index 7 in the embedding matrix. Then to perform lookup from this matrix we need to pass to embedding lookup table index 7. This Vocabulary will convert the word 'scientist' to number 7. We also restrict the maximum number of words to 10000 to make learning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "# Number of tokens in the vocabulary\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Dictionary class. Each instance holds tags or tokens or characters and provides\n",
    "# dictionary like functionality like indices to tokens and tokens to indices.\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens, max_tokens=10000):\n",
    "        # We set default ind to position of <UNK>\n",
    "        self._t2i = defaultdict(lambda: 0)\n",
    "        self._i2t = list()\n",
    "        self.frequencies = Counter(tokens)\n",
    "        self.counter = 0\n",
    "        # The token with 0 index will be unknow token (or out of vocabulary token)\n",
    "        self._add_token('<UNK>', 2**30)\n",
    "        for token, freq in self.frequencies.most_common(max_tokens - 1):\n",
    "            self._add_token(token, freq)\n",
    "    \n",
    "    def _add_token(self, token, frequency):\n",
    "        self._t2i[token] = self.counter\n",
    "        self.frequencies[token] += 0\n",
    "        self._i2t.append(token)\n",
    "        self.counter += 1\n",
    "\n",
    "    def idx2tok(self, idx):\n",
    "        return self._i2t[idx]\n",
    "\n",
    "    def idxs2toks(self, idxs, filter_paddings=False):\n",
    "        toks = []\n",
    "        for idx in idxs:\n",
    "            if not filter_paddings or idx != self.tok2idx('<PAD>'):\n",
    "                toks.append(self._i2t[idx])\n",
    "        return toks\n",
    "\n",
    "    def tok2idx(self, tok):\n",
    "        return self._t2i[tok]\n",
    "\n",
    "    def toks2idxs(self, toks):\n",
    "        return [self._t2i[tok] for tok in toks]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._t2i[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._t2i\n",
    "    \n",
    "vocab = Vocabulary(train_set, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator\n",
    "\n",
    "To train the network we have to pass batches of examples to it. In our case we have to pass number of 'sentences' to the network. Since the dataset has no punctuation we will split the total sequence of tokens into subsequences of given length and pass the them to the network. Typical length of 'sentence' (subsequence) is around 35 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    sentence_len,\n",
    "                    data_type='train',\n",
    "                    shuffle=True,\n",
    "                    allow_smaller_last_batch=True):\n",
    "    if data_type == 'train':\n",
    "        tokens = train_set\n",
    "    elif data_type == 'valid':\n",
    "        tokens = valid_set\n",
    "    else:\n",
    "        tokens = test_set\n",
    "    sentences = []\n",
    "    for n in range(len(tokens) // sentence_len):\n",
    "        sentences.append(tokens[n * sentence_len: (n + 1) * sentence_len])\n",
    "    n_samples = len(sentences)\n",
    "    \n",
    "    if shuffle and data_type == 'train':\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        x_batch = np.zeros([batch_end - batch_start, sentence_len], dtype=np.int32)\n",
    "        for ind, n in enumerate(range(batch_start, batch_end)):\n",
    "            sentence = sentences[order[n]]\n",
    "            sentence_idxs = vocab.toks2idxs(sentence)\n",
    "            x_batch[ind] = sentence_idxs\n",
    "        yield x_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sententence indices:\n",
      "[402   0  11 108   1 130 215  12  45  51]\n",
      "Sententence words:\n",
      "['white', '<UNK>', 'is', 'made', 'the', 'same', 'way', 'as', 'its', 'more']\n",
      "Sententence indices:\n",
      "[ 426 5196  402    0    1    0   26 8128    3   61]\n",
      "Sententence words:\n",
      "['famous', 'cousin', 'white', '<UNK>', 'the', '<UNK>', 'are', 'crushed', 'and', 'after']\n"
     ]
    }
   ],
   "source": [
    "batch = next(batch_generator(2, 10, 'test'))\n",
    "\n",
    "for sentence_idxs in batch:\n",
    "    print('Sententence indices:')\n",
    "    print(sentence_idxs)\n",
    "    print('Sententence words:')\n",
    "    print(vocab.idxs2toks(sentence_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "Now we will assemble the ELMO network. As a basis we will use GRU units as a faster an less prone to overfitting solution.\n",
    "Following the paper the network will contain: embedding layer, two recurrent layers, and output prediction layer. The embedding layer will convert indices of words to corresponding vectors. Two recurrent layers will contextualize the word representations. The output layer will project the hidden states produced by recurrent neural network to probability scores of next token. The dimensionality of the output is equal to vocabulary size. So, for each token we get the probobality (how likely to see this token as the next token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class ELMO(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 n_hidden,\n",
    "                 embedding_dim):\n",
    "        super(ELMO, self).__init__()\n",
    "\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Two forward layers\n",
    "        self.gru_fw_1 = torch.nn.GRU(embedding_dim, n_hidden)\n",
    "        self.gru_fw_2 = torch.nn.GRU(n_hidden, n_hidden)\n",
    "        \n",
    "        # Two backward layers\n",
    "        self.gru_bw_1 = torch.nn.GRU(embedding_dim, n_hidden)\n",
    "        self.gru_bw_2 = torch.nn.GRU(n_hidden, n_hidden)\n",
    "        \n",
    "        self.output_projection = nn.Linear(n_hidden, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch, direction):\n",
    "        \"\"\" Compute forward pass in given direction\n",
    "        Args:\n",
    "            batch: is a torch Variable [batch_size x sentence_length] (int64)\n",
    "            direction: either 'forward' of 'backward'\n",
    "        Returns:\n",
    "            logits: log probabilities for the given direction\n",
    "        \"\"\"\n",
    "        # Sequences for backward direction assumed to be reversed\n",
    "        batch_size, seq_len = batch.size()\n",
    "        # We drop last token in order to predict every following token\n",
    "        emb = self.embed(batch[:, :-1])\n",
    "        output_embeddings = emb\n",
    "        \n",
    "        # Following dicumentation input to GRU has the following order (seq_len, batch, input_size)\n",
    "        emb = emb.permute([1, 0, 2])\n",
    "        \n",
    "        # Forward direction\n",
    "        if direction == 'forward':\n",
    "            units, _ = self.gru_fw_1(emb)\n",
    "            units, _ = self.gru_fw_2(units)\n",
    "            \n",
    "        # Backward direction\n",
    "        else:\n",
    "            units, _ = self.gru_bw_1(emb)\n",
    "            units, _ = self.gru_bw_2(units)\n",
    "        units = self.output_projection(units)\n",
    "        logits = self.output_layer(units)\n",
    "        \n",
    "        return logits.permute([1, 0, 2])\n",
    "    \n",
    "    # For using as a pretrained model\n",
    "    def embed_batch(self, batch_fw, batch_bw):\n",
    "        \"\"\"This method should be used to pass embeddings of pre-trained model another task\n",
    "        Args:\n",
    "            batch_fw - tensor of indices of words with size [batch_size, seq_len] (int64)\n",
    "            batch_fw - tensor of indices of words with reverse order, size [batch_size, seq_len] (int64)\n",
    "        \n",
    "        Returns:\n",
    "            units_0, units_1, units_2 - units from token level, first RNN layer, and second rnn layer\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_len = batch_fw.size()\n",
    "        # We drop last token in order to predict every following token\n",
    "        emb_fw = self.embed(batch_fw)\n",
    "        emb_bw = self.embed(batch_bw)\n",
    "        \n",
    "        # Following dicumentation input to GRU has the following order (seq_len, batch, input_size)\n",
    "        emb_fw_perm = emb_fw.permute([1, 0, 2])\n",
    "        emb_bw_perm = emb_bw.permute([1, 0, 2])\n",
    "        \n",
    "        # Forward direction\n",
    "        units_fw_1, _ = self.gru_fw_1(emb_fw_perm)\n",
    "        units_fw_2, _ = self.gru_fw_2(units_fw_1)\n",
    "        units_fw_1 = units_fw_1.permute([1, 0, 2])\n",
    "        units_fw_2 = units_fw_2.permute([1, 0, 2])\n",
    "\n",
    "        # Backward direction\n",
    "        units_bw_1, _ = self.gru_bw_1(emb_bw_perm)\n",
    "        units_bw_2, _ = self.gru_fw_2(units_bw_1)\n",
    "        units_bw_1 = units_bw_1.permute([1, 0, 2])\n",
    "        units_bw_2 = units_bw_2.permute([1, 0, 2])\n",
    "\n",
    "        # Create representaions for each layer\n",
    "        units_0 = torch.cat([emb_fw, torch.zeros_like(units_fw_1), torch.zeros_like(units_bw_1)], 2)\n",
    "        units_1 = torch.cat([emb_fw, units_fw_1, units_bw_1], 2)\n",
    "        units_2 = torch.cat([emb_fw, units_fw_2, units_bw_2], 2)\n",
    "        \n",
    "        return units_0, units_1, units_2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the instance of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELMO(\n",
       "  (embed): Embedding(10000, 256)\n",
       "  (gru_fw_1): GRU(256, 256)\n",
       "  (gru_fw_2): GRU(256, 256)\n",
       "  (gru_bw_1): GRU(256, 256)\n",
       "  (gru_bw_2): GRU(256, 256)\n",
       "  (output_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "net = ELMO(VOCAB_SIZE, hidden_size, embedding_dim)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create supplimentary function to feed pytorch Variables into the network. It will prepare forward and backward directions inputs and outputs. The backward direction the input is just the reversed sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(x):\n",
    "    x = x.astype(np.int64)\n",
    "    np_x_fw = x\n",
    "    np_x_bw = np.array(x[:, ::-1])\n",
    "    x_fw = Variable(torch.from_numpy(np_x_fw)).cuda()\n",
    "    x_bw = Variable(torch.from_numpy(np.array(np_x_bw))).cuda()\n",
    "\n",
    "    y_fw = Variable(torch.from_numpy(np_x_fw[:, 1:])).cuda()\n",
    "    y_bw = Variable(torch.from_numpy(np_x_bw[:, 1:])).cuda()\n",
    "    return x_fw, x_bw, y_fw, y_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "We will train the network for 10 epochs. After each epoch we will perform validation. And at the end of the training we will perform testing. The log loss will be displayed during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: [5.716759]\n",
      "Train loss: [5.249713]\n",
      "Train loss: [5.0771194]\n",
      "Train loss: [4.9769306]\n",
      "Train loss: [4.9078603]\n",
      "Train loss: [4.857866]\n",
      "Validation loss: [4.841934]\n",
      "Epoch: 1\n",
      "Train loss: [4.7844253]\n",
      "Train loss: [4.7626805]\n",
      "Train loss: [4.7430506]\n",
      "Train loss: [4.7212906]\n",
      "Train loss: [4.7104015]\n",
      "Train loss: [4.690979]\n",
      "Validation loss: [4.725791]\n",
      "Epoch: 2\n",
      "Train loss: [4.6198497]\n",
      "Train loss: [4.6301255]\n",
      "Train loss: [4.630003]\n",
      "Train loss: [4.624859]\n",
      "Train loss: [4.619018]\n",
      "Train loss: [4.6117063]\n",
      "Validation loss: [4.675135]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "sequence_len = 35\n",
    "batch_size = 64\n",
    "print_loss_every = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "for k in range(n_epochs):\n",
    "    sum_loss = 0\n",
    "    print('Epoch: {}'.format(k))\n",
    "    for n, x in enumerate(batch_generator(batch_size, sequence_len, 'train')):\n",
    "        x_fw, x_bw, y_fw, y_bw = prepare_batch(x)\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # Forward direction\n",
    "        logits_fw = net.forward(x_fw, 'forward')\n",
    "        logits_fw = nn.functional.log_softmax(logits_fw, 2)\n",
    "        loss_fw = loss_function(logits_fw.permute([0, 2, 1]), y_fw)\n",
    "        \n",
    "        # Backward direction\n",
    "        logits_bw = net.forward(x_bw, 'backward')\n",
    "        logits_bw = nn.functional.log_softmax(logits_bw, 2)\n",
    "        loss_bw = loss_function(logits_bw.permute([0, 2, 1]), y_bw)\n",
    "        \n",
    "        loss = (loss_fw + loss_bw) / 2\n",
    "        loss_val = loss.cpu().data.numpy()\n",
    "        sum_loss += loss_val\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if n % print_loss_every == print_loss_every - 1:\n",
    "            print('Train loss: {}'.format(sum_loss / print_loss_every))\n",
    "            sum_loss = 0\n",
    "            \n",
    "    # Affects only batch_norm and dropout layers\n",
    "    net.eval()        \n",
    "        \n",
    "    sum_loss = 0\n",
    "    for n, x in enumerate(batch_generator(batch_size, sequence_len, 'valid')):\n",
    "        x_fw, x_bw, y_fw, y_bw = prepare_batch(x)\n",
    "        # Forward direction\n",
    "        logits_fw = net.forward(x_fw, 'forward')\n",
    "        logits_fw = nn.functional.log_softmax(logits_fw, 2)\n",
    "        loss_fw = loss_function(logits_fw.permute([0, 2, 1]), y_fw)\n",
    "        \n",
    "        # Backward direction\n",
    "        logits_bw = net.forward(x_bw, 'backward')\n",
    "        logits_bw = nn.functional.log_softmax(logits_bw, 2)\n",
    "        loss_bw = loss_function(logits_bw.permute([0, 2, 1]), y_bw)\n",
    "        \n",
    "        loss = (loss_fw + loss_bw) / 2\n",
    "        loss_val = loss.cpu().data.numpy()\n",
    "        sum_loss += loss_val\n",
    "    print('Validation loss: {}'.format(sum_loss / n))\n",
    "    \n",
    "    # Affects only batch_norm and dropout layers\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to sample from the model forward direction network given short initial phrase like \"the most\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'most', 'lowest', 'prominent', 'and', 'rugby', 'popular', 'league', 'team', '<UNK>', 'are', 'ranked']\n"
     ]
    }
   ],
   "source": [
    "x = [vocab.toks2idxs('the most'.split())]\n",
    "for n in range(10):\n",
    "    x_fw = Variable(torch.from_numpy(np.array(x))).cuda()\n",
    "    logits_fw = net.forward(x_fw, 'forward')\n",
    "    logits = nn.functional.log_softmax(logits_fw, 2).cpu().data.numpy()\n",
    "    logits = logits[0][-1]\n",
    "    # Softmax\n",
    "    p = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    # Sample from obtained probability distribution\n",
    "    new_tok_ind = np.argmax(np.random.multinomial(1, p - 1e-9))\n",
    "    x[0].append(new_tok_ind)\n",
    "\n",
    "print(vocab.idxs2toks(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: [4.580204]\n"
     ]
    }
   ],
   "source": [
    "sum_loss = 0\n",
    "for n, x in enumerate(batch_generator(batch_size, sequence_len, 'test')):\n",
    "    x_fw, x_bw, y_fw, y_bw = prepare_batch(x)\n",
    "    # Forward direction\n",
    "    logits_fw = net.forward(x_fw, 'forward')\n",
    "    logits_fw = nn.functional.log_softmax(logits_fw, 2)\n",
    "    loss_fw = loss_function(logits_fw.permute([0, 2, 1]), y_fw)\n",
    "\n",
    "    # Backward direction\n",
    "    logits_bw = net.forward(x_bw, 'backward')\n",
    "    logits_bw = nn.functional.log_softmax(logits_bw, 2)\n",
    "    loss_bw = loss_function(logits_bw.permute([0, 2, 1]), y_bw)\n",
    "\n",
    "    loss = (loss_fw + loss_bw) / 2\n",
    "    loss_val = loss.cpu().data.numpy()\n",
    "    sum_loss += loss_val\n",
    "print('Test loss: {}'.format(sum_loss / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "#### Extend the model\n",
    "Try this additional features to get loss on test lower than 4.0:\n",
    "- Dropout (typically added to the embeddings and before output layer) https://arxiv.org/abs/1708.02182\n",
    "- Gradient clipping to prevent exploding gradients\n",
    "- Tied output weights (input embedding matrix can be used for output layer) https://arxiv.org/abs/1611.01462\n",
    "- larger hidden sizes for rnn and hidden sizes\n",
    "- LSTM instead of GRU\n",
    "- Inialize the rucurrent units with trainable states\n",
    "- Pre-trained word embeddings (for instance [GloVe](https://nlp.stanford.edu/projects/glove/) or [FastText](https://github.com/facebookresearch/fastText))\n",
    "- Compute [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the model\n",
    "\n",
    "\n",
    "#### Try it on real data\n",
    "\n",
    "Pretrain the language model on the large dataset, for example [Amazon reviews](https://www.kaggle.com/bittlingmayer/amazonreviews/data), and implement the pre-trained model for  [IMDB sentiment analisis task](http://ai.stanford.edu/~amaas/data/sentiment/). For this task build the network that takes contextualized embeddings poduced by ELMO and pass them to another network. Use embed_batch method of the network class to obtain ELMO embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
