{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow keras matplotlib tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy vs Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.random((5000, 5000))\n",
    "B = np.random.random((5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_np = A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# задаем входы для матриц\n",
    "A_input = tf.placeholder('float32', shape=(5000, 5000), name='A')\n",
    "B_input = tf.placeholder('float32', shape=(5000, 5000), name='B')\n",
    "\n",
    "output = A_input @ B_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_tf = output.eval({A_input: A,\n",
    "                         B_input: B}, session=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(result_np, result_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = # вектор\n",
    "v2 = # вектор\n",
    "\n",
    "mse = # mse \n",
    "\n",
    "compute_mse = lambda vector1, vector2: # возвращает число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el, el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print(n, \"Что-то не так\", true_mse, my_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заводим переменную\n",
    "shared_vector_1 = tf.Variable(initial_value=np.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем в графе\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"initial value\",s.run(shared_vector_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#меняем значение\n",
    "s.run(shared_vector_1.assign(np.arange(5)))\n",
    "print (\"new value\", s.run(shared_vector_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scalar = tf.placeholder('float32')\n",
    "\n",
    "scalar_squared = my_scalar ** 2\n",
    "\n",
    "derivative = tf.gradients(scalar_squared, my_scalar)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared, x_squared_der = s.run([scalar_squared,derivative],\n",
    "                                 {my_scalar:x})\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector = tf.placeholder('float32',[None])\n",
    "\n",
    "weird_psychotic_function = tf.reduce_mean((my_vector + my_scalar) ** (1 + tf.nn.moments(my_vector, [0])[1]) + 1./ tf.atan(my_scalar))/(my_scalar ** 2 + 1) + 0.01 * tf.sin(2 * my_scalar ** 1.5) * (tf.reduce_sum(my_vector) * my_scalar ** 2) * tf.exp((my_scalar-4) ** 2) / (1 + tf.exp((my_scalar - 4) ** 2)) * (1. - (tf.exp(-(my_scalar - 4) ** 2)) / (1 + tf.exp(-(my_scalar - 4) ** 2))) ** 2\n",
    "\n",
    "\n",
    "der_by_scalar, der_by_vector = # градиенты по переменным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting your derivative\n",
    "scalar_space = np.linspace(1,7,100)\n",
    "\n",
    "y = [s.run(weird_psychotic_function,{my_scalar:x,my_vector:[1,2,3]})\n",
    "     for x in scalar_space]\n",
    "\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "\n",
    "y_der_by_scalar = [s.run(der_by_scalar,{my_scalar:x,my_vector:[1,2,3]})\n",
    "     for x in scalar_space]\n",
    "\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизаторы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_guess = tf.Variable(np.zeros(2,dtype='float32'))\n",
    "y_true = tf.range(1,3,dtype='float32')\n",
    "\n",
    "loss = tf.reduce_mean((y_guess - y_true + tf.random_normal([2]))**2) \n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss, var_list=y_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "guesses = [s.run(y_guess)]\n",
    "\n",
    "for _ in range(100):\n",
    "    s.run(optimizer)\n",
    "    guesses.append(s.run(y_guess))\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.plot(*zip(*guesses),marker='.')\n",
    "    plt.scatter(*s.run(y_true),c='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = # веса\n",
    "input_X = # картинки \n",
    "input_y = # ответы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = # предсказанные вероятности \n",
    "loss = # функция потерь\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss, var_list=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function = lambda X_b, y_b: # функция обучения, возвращает loss\n",
    "predict_function = lambda X_b: # функция тестирования, возвращает предсказанные вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    loss_i = train_function(X_train, y_train)\n",
    "    \n",
    "    print (\"loss at iter %i:%.4f\"%(i,loss_i))\n",
    "    \n",
    "    print (\"train auc:\", roc_auc_score(y_train, predict_function(X_train)))\n",
    "    print (\"test auc:\", roc_auc_score(y_test, predict_function(X_test)))\n",
    "\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(weights.eval().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN:\n",
    "    def __init__(self):\n",
    "        self.training  = tf.placeholder_with_default(True, shape=())\n",
    "        self.dense_1 = # полносвязный слой\n",
    "        self.dense_2 = # последний полносвязный слой \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.dense_2(tf.layers.Dropout()(self.dense_1(X), training=self.training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = DenseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "# np.array\n",
    "train_data = mnist.train.images \n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images \n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[-1].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = tf.placeholder('float32', shape=(None, ) + train_data.shape[1:])\n",
    "input_y = tf.placeholder('int32', shape=(None, ))\n",
    "\n",
    "nn = DenseNN()\n",
    "\n",
    "# хотим получать не вероятности, а значения до softmax\n",
    "logits = nn(input_X)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(input_y, logits)\n",
    "\n",
    "predictions = {\"classes\": tf.argmax(input=logits, axis=1),\n",
    "               \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss=loss,\n",
    "                              global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "idx = np.arange(len(train_data))\n",
    "for epoch in range(10):\n",
    "    np.random.shuffle(idx)\n",
    "    train_loss = 0\n",
    "    cnt = 0\n",
    "    for i in tqdm_notebook(range(0, len(train_data), batch_size)):\n",
    "        cur_idx = idx[i: i + batch_size]\n",
    "        cur_loss = s.run([loss, train_op], {input_X: train_data[idx], \n",
    "                                            input_y: train_labels[idx],\n",
    "                                            nn.training: True})[0]\n",
    "        train_loss += cur_loss\n",
    "        cnt += 1\n",
    "        \n",
    "        if i % 64 == 63:\n",
    "            print(train_loss / cnt)\n",
    "        \n",
    "    \n",
    "        \n",
    "    eval_loss = s.run(loss, {input_X: eval_data, \n",
    "                             input_y: eval_labels,\n",
    "                             nn.training: False})\n",
    "    classes = s.run(predictions['classes'], {input_X: eval_data, \n",
    "                                             input_y: eval_labels,\n",
    "                                             nn.training: False})\n",
    "    print(np.mean(classes == eval_labels))\n",
    "    \n",
    "    print(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_shape=train_data.shape[1:], units=10, activation='relu'))\n",
    "model.add(Dense(input_shape=train_data.shape[1:], units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, batch_size=64, epochs=10, validation_data=(eval_data, eval_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q \"https://s1.1zoom.ru/big0/607/359313-sepik.jpg\" -O dog.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = image.load_img('./dog.jpg', target_size=(224, 224))\n",
    "plt.imshow(dog)\n",
    "\n",
    "img = image.img_to_array(dog)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = preprocess_input(img)\n",
    "\n",
    "preds = resnet.predict(img)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.layers[-1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 16, input_length=max_review_length))\n",
    "model.add(SimpleRNN(10))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    " * Доучите любую модель из keras.applications на бинарную классификацию на данных https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition и сделайте посылку в рейтинговую систему. В качестве отчета присылайте свой код и посылку на kaggle. \n",
    " \n",
    " Подсказка - доучивать на датасете всю сеть целиком долго, плюс она переобучится.\n",
    " * Реализуйте сверточную нейронную сеть на данных IMDB. Сравните по качеству и времени работы с рекуррентной сетью выше при примерно равном числе параметров. В качестве отчета присылайте свой код с обучением моделей и обсуждение итогов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
