{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для воспроизводимости\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN intro\n",
    "\n",
    "Давайте разберемся что из себя вообще представляют рекуррентные нейронные сети в самом простом виде.\n",
    "\n",
    "<img src=\"./pics/rnn.png\" width=\"90%\">\n",
    "\n",
    "В самом простом виде для одного входного вектора $x_{(t)}$ и одного слоя рекуррентной сети справедливо такое соотношение:\n",
    "\n",
    "$$y_{(t)} = \\phi (x_{(t)}^T \\cdot w_x + y_{(t-1)}^T \\cdot w_y + b)$$\n",
    "\n",
    "где \n",
    "* $x(t)$ -- входной вектор на текущем шаге\n",
    "* $y(t)$ -- выходной вектор на текущем шаге\n",
    "* $w_x$ -- вектор весов нейронов для входа\n",
    "* $w_y$ -- вектор весов нейронов для выхода\n",
    "* $y(t-1)$ -- выходной вектор с прошлого шага. Для шага 0 этот вектор нулевой\n",
    "* $b$ -- байес (bias)\n",
    "* $\\phi$ -- обозначение для функции активации, например ReLU\n",
    "\n",
    "\n",
    "Есть понятие **hidden_state** ( $h(t)$ ) -- это \"память\" рекуррентной ячейки.\n",
    "\n",
    "В общем случае $h_{(t)} = f(h_{(t-1)}, x_{(t)})$, но выход также $y{(t)} = f(h{(t-1)}, x{(t)})$.\n",
    "\n",
    "В данном случае $h(t) == y(t)$, но на практике используются более сложные архитектуры и в них **hidden_state** не совпадает с непосредственным выходом нейросетки.\n",
    "\n",
    "------\n",
    "\n",
    "## Напишем свою простую RNN сеть\n",
    "\n",
    "Снова немножко математики чтобы привести формулу выше к более удобному виду.\n",
    "\n",
    "Представим, что на вход подается не один вектор $x_{(t)}$, а целый мини-батч размера $m$ таких векторов $X_{(t)}$, соответственно все дальнейшие размышления мы уже производим в матричном виде:\n",
    "\n",
    "$$ Y_{(t)} = \\phi(X_{(t)}^T \\cdot W_x + Y_{(t-1)}^T \\cdot W_y + b) = \\phi([X_{(t)} Y_{(t-1)}] \\cdot W + b) $$\n",
    "где\n",
    "$$ W = [W_x W_y]^T $$\n",
    "\n",
    "*Операция в скобках квадратных -- конкатенация матриц\n",
    "\n",
    "По размерностям:\n",
    "* $Y_{(t)}$ -- матрица [$m$ x n_neurons]\n",
    "* $X_{(t)}$ -- матрица [$m$ x n_features]\n",
    "* $b$ -- вектор длины n_neurons\n",
    "* $W_x$ -- веса между входами и нейронами размерностью [n_features x n_neurons]\n",
    "* $W_y$ -- веса связей с прошлым выходом размерностью [n_neurons x n_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Напишем нейронку прямо как на картинке в самом верху с 5-ю нейронами\n",
    "# На вход будем подавать векторы длины 3\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "# С текушей имплементацией наша нейронка делает всего 2 шага\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_features, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# Здесь в качестве функции phi берем гиперболический тангенс\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Будем подавать на вход мини батчи размером 4\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])  # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])  # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0664006 ,  0.96257669,  0.68105787,  0.70918542, -0.89821595],\n",
       "       [ 0.9977755 , -0.71978885, -0.99657625,  0.9673925 , -0.99989718],\n",
       "       [ 0.99999774, -0.99898815, -0.99999893,  0.99677622, -0.99999988],\n",
       "       [ 1.        , -1.        , -1.        , -0.99818915,  0.99950868]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        , -1.        ,  0.40200216, -1.        ],\n",
       "       [-0.12210433,  0.62805319,  0.96718419, -0.99371207, -0.25839335],\n",
       "       [ 0.99999827, -0.9999994 , -0.9999975 , -0.85943311, -0.9999879 ],\n",
       "       [ 0.99928284, -0.99999815, -0.99990582,  0.98579615, -0.92205751]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Напишите то же самое, но использовав всего одно матричное перемножение на каждом шаге (см формулу в объяснении выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "< здесь код объявления новых переменных, а также построения графа вычислений >\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val_1, Y1_val_1 = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic_rnn\n",
    "\n",
    "В tf есть функция `tf.contrib.rnn.static_rnn` которая создает для каждого unrolling'а (т.е. каждого батча на входе) отдельную ячейку того типа, который мы ей передадим. В данном случае наша имплементация совпадает с имплементацией `tf.contrib.rnn.BasicRNNCell` в tf. Это все не очень круто, если требуется сделать большое число шагов -- у нас попросту может закончится память, если мы вдруг решим делать backprop. Поэтому к счастью есть другой путь -- это `dynamic_rnn`.\n",
    "\n",
    "Сделаем весь предыдущий пример с помощью `dynamic_rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "# параметр с истинной длиной последовательностей\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 5)\n",
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val.shape)\n",
    "print(states_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.68579948 -0.25901747 -0.80249101 -0.18141513 -0.37491536]\n",
      "  [-0.99996698 -0.94501185  0.98072106 -0.9689762   0.99966913]]\n",
      "\n",
      " [[-0.99099374 -0.64768541 -0.67801034 -0.7415446   0.7719509 ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.99978048 -0.85583007 -0.49696958 -0.93838578  0.98505187]\n",
      "  [-0.99951065 -0.89148796  0.94170523 -0.38407657  0.97499216]]\n",
      "\n",
      " [[-0.02052618 -0.94588047  0.99935204  0.37283331  0.9998163 ]\n",
      "  [-0.91052347  0.05769409  0.47446665 -0.44611037  0.89394671]]]\n"
     ]
    }
   ],
   "source": [
    "# заметим, что у второго инпута есть нули в final state\n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99996698 -0.94501185  0.98072106 -0.9689762   0.99966913]\n",
      " [-0.99099374 -0.64768541 -0.67801034 -0.7415446   0.7719509 ]\n",
      " [-0.99951065 -0.89148796  0.94170523 -0.38407657  0.97499216]\n",
      " [-0.91052347  0.05769409  0.47446665 -0.44611037  0.89394671]]\n"
     ]
    }
   ],
   "source": [
    "# а тут уже нет нулей;\n",
    "# так происходит благодаря наличию параметра seq_length\n",
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация имен\n",
    "\n",
    "А теперь попробуем понять что можно со всем этим вышеперечисленным делать полезного.\n",
    "\n",
    "_Teaser:_\n",
    "\n",
    "* Сложно придумать имя для переменной? Но куда сложнее придумать хорошее имя для человека.\n",
    "  Поэтому давайте напишем нейронку, которая сделает это за нас.\n",
    "* Набор данных содержит ~ 8 тыс человеческих имен из разных культур [в латинской расшифровке]\n",
    "* Цель (игрушечная проблема): изучить генеративную модель по именам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      "Abagael\n",
      "Claresta\n",
      "Glory\n",
      "Liliane\n",
      "Prissie\n",
      "Geeta\n",
      "Giovanne\n",
      "Piggy\n"
     ]
    }
   ],
   "source": [
    "print('n samples = ', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x.strip().capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текста\n",
    "\n",
    "Рассмотрим все буквы без учета регистра + ')' -- конец имени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  30\n"
     ]
    }
   ],
   "source": [
    "# Найдем все уникальные символы\n",
    "# Без учета регистра\n",
    "\n",
    "token_set = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        token_set.add(letter)\n",
    "\n",
    "\n",
    "token_set.add(')')\n",
    "tokens = list(token_set)\n",
    "tokens.sort()\n",
    "\n",
    "print('n_tokens = ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!token_to_id = < словарь символов -> их id (index в tokens list)>\n",
    "token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "#!id_to_token = < словарь айдишников -> соответствующие символы >\n",
    "id_to_token = {i: t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим распределение длин всех имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f789438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len, names)))\n",
    "\n",
    "# Посмотрим какая максимальная длина у имени в этом датасете\n",
    "MAX_LEN = min([60, max(list(map(len, names)))])-1\n",
    "\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переведем все символы в их id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get, name + ')')), names))\n",
    "\n",
    "\n",
    "# Добьем нулями короткие имена до MAX_LEN и усечем слишком длинные\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN+1] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN+1:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN+1 - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len, names_ix))) == 1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  5,  4, 10,  4,  8, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  4, 10,  4, 12, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5,  8, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 12,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4,  8, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4, 12, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4, 15,  8,  2,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_ix[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерилка батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0, len(data), size=batch_size)]\n",
    "    x = rows[:, :-1]\n",
    "    y = rows[:, 1:]\n",
    "    \n",
    "    count = lambda r: np.sum([id_to_token[t] != ' ' for t in r])\n",
    "    lengths = list(map(count, x))\n",
    "    \n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, length = sample_batch(names_ix, 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 21, 18, 17,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 13, 18, 28,  6,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 14,  4, 23, 11,  4, 21, 28, 17,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0, 16,  4, 24,  7, 12,  8,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 12, 21, 25, 12, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 16,  4, 21, 17, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  6, 18, 21, 21,  8, 17,  4,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 25, 12,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 10, 24,  8, 17,  8, 25,  8, 21,  8,  2,  0,  0,  0,  0],\n",
       "       [ 0, 22, 11,  4, 28, 17,  8,  2,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 18, 17,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [13, 18, 28,  6,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14,  4, 23, 11,  4, 21, 28, 17,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [16,  4, 24,  7, 12,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12, 21, 25, 12, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [16,  4, 21, 17, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 6, 18, 21, 21,  8, 17,  4,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [25, 12,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10, 24,  8, 17,  8, 25,  8, 21,  8,  2,  0,  0,  0,  0,  0],\n",
       "       [22, 11,  4, 28, 17,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 9, 7, 6, 6, 8, 4, 10, 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Входы сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# подразумевается, что размерность X [batch_size, max_length];\n",
    "X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neurons = 60\n",
    "embedding_size = 8\n",
    "vocabulary_size = len(tokens)\n",
    "\n",
    "n_steps = MAX_LEN # Этот параметр совпадает\n",
    "                  # с максимальной длиной последовательности, \n",
    "                  # которая может быть подана на вход\n",
    "                  # иначе говоря, это unrollings\n",
    "\n",
    "# для входной последовательности создаем матрицу эмбеддингов\n",
    "embedding_mtx = tf.get_variable(name='embeddings', shape=[vocabulary_size, embedding_size])\n",
    "\n",
    "# достаем из матрицы эмбеддингов нужные нам векторы X\n",
    "embed = tf.nn.embedding_lookup(embedding_mtx, X)\n",
    "\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.tanh)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell=cell, inputs=embed,\n",
    "                                        sequence_length=lengths, dtype=tf.float32)\n",
    "\n",
    "# получаем ненормированное распределение по классам \n",
    "# для каждого анроллинга в каждом сэмле в батче\n",
    "pred_logits = tf.layers.dense(inputs=rnn_outputs, units=vocabulary_size, name='output_projection')\n",
    "\n",
    "# кодируем one-hot классы, т.к. это тоже нужно функции лосса\n",
    "labels_one_hot = tf.one_hot(y, depth=vocabulary_size, dtype=tf.float32)\n",
    "\n",
    "# считаем илололосс\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=labels_one_hot,\n",
    "    logits=pred_logits)\n",
    "    \n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "\n",
    "pred_probas = tf.nn.softmax(pred_logits)\n",
    "\n",
    "# берем максимум по оси, соответствующей количеству классов\n",
    "# получаем матрицу размера [batch_size, num_steps]\n",
    "prediction = tf.argmax(pred_probas, axis=2)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate_ph).minimize(loss)\n",
    "\n",
    "# берем распределение вероятностей только для последнего символа в каждом сэмпле\n",
    "# это потребуется для генерации\n",
    "last_word_probas = pred_probas[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как можно узнать о параметрах, которые тренируются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embeddings:0' shape=(30, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/basic_rnn_cell/weights:0' shape=(68, 60) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/basic_rnn_cell/biases:0' shape=(60,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_projection/kernel:0' shape=(60, 30) dtype=float32_ref>,\n",
       " <tf.Variable 'output_projection/bias:0' shape=(30,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем функцию, позволяющую генерить имена по затравке\n",
    "\n",
    "*Что делается*\n",
    "\n",
    "* Берется затравка (seed_phrase)\n",
    "* Предсказывается вероятность появления следующего токена\n",
    "* Следующий токен сэмплируется из распределения, предсказанного моделью\n",
    "* Полученный токен добавляется к затравке\n",
    "* Повторяем с шага 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(sess, seed_phrase=None, N=MAX_LEN, n_snippets=1):\n",
    "    \n",
    "    if seed_phrase is None:\n",
    "        seed_phrase = ' '\n",
    "    elif seed_phrase[0].isalpha():\n",
    "        seed_phrase = ' ' + seed_phrase\n",
    "    seed_phrase = seed_phrase.lower()\n",
    "    seed_phrase = np.array([token_to_id[tok] for tok in seed_phrase])\n",
    "    L = len(seed_phrase)\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        x = np.zeros(N)\n",
    "        x[:len(seed_phrase)] = seed_phrase\n",
    "        for n in range(N - L):\n",
    "            feed_dict = {X: x[:L + n].reshape([1, -1]), lengths: [len(x)]}\n",
    "            p = sess.run(last_word_probas, feed_dict=feed_dict).reshape(-1)\n",
    "            ix = np.random.choice(np.arange(len(tokens)), p=p)\n",
    "            x[L + n] = ix\n",
    "        snippet = ''.join([id_to_token[idx] for idx in x])\n",
    "        if ')' in snippet:\n",
    "            upto = snippet.index(')')\n",
    "            snippet = snippet[:upto]\n",
    "        snippets.append(snippet.strip().capitalize())\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_pred(y_pred, k = 3):\n",
    "    \"\"\"\n",
    "    k: сколько вывести \n",
    "    предсказаний модели среди всех y_pred\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(k):\n",
    "        print(\"\".join( [id_to_token[t] for t in y_pred[i,:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Generated:  [\"Iz'm\", 'Gmx uz', 'Szelmhazvjoblr', 'Lzogsaggjtltof', 'Uddle pkjynvan', 'Wnrg']\n",
      "-------\n",
      "\n",
      "EPOCH:  0\n",
      "AVERAGE LOSS:  1.62943659818\n",
      ">>Predicted: \n",
      "aesni)         \n",
      "anrna)))       \n",
      "aerlnae)       \n",
      ">>Generated:  ['Erdisa', 'Eych', 'Eshie', 'Angostia', 'Astelie', 'Mesamet']\n",
      "-------\n",
      "\n",
      "EPOCH:  1\n",
      "AVERAGE LOSS:  1.09818096185\n",
      ">>Predicted: \n",
      "conii))e       \n",
      "chardh))))     \n",
      "cosi           \n",
      ">>Generated:  ['Welisa', 'Jolia', 'Meto', 'Addor', 'Gresarde', 'Gwedy']\n",
      "-------\n",
      "\n",
      "EPOCH:  2\n",
      "AVERAGE LOSS:  1.05015222597\n",
      ">>Predicted: \n",
      "cart)i))       \n",
      "cld)a          \n",
      "caren          \n",
      ">>Generated:  ['Marte', 'Costsa', 'Chento', 'Guaste', 'Patulee', 'Marista']\n",
      "-------\n",
      "\n",
      "EPOCH:  3\n",
      "AVERAGE LOSS:  1.03293791163\n",
      ">>Predicted: \n",
      "arristaaa)     \n",
      "aala           \n",
      "aanii)         \n",
      ">>Generated:  ['Karrobelneot', 'Kalasina', 'Marche', 'Amoosta', 'Dieturah', 'Sisuinae']\n",
      "-------\n",
      "\n",
      "EPOCH:  4\n",
      "AVERAGE LOSS:  1.01994612098\n",
      ">>Predicted: \n",
      "meenn          \n",
      "mariin)        \n",
      "maldi)         \n",
      ">>Generated:  ['Ckri-', 'Marcletta', 'Ally', 'Jossen', 'Ydamera', 'Davie']\n"
     ]
    }
   ],
   "source": [
    "s = tf.Session()\n",
    "    \n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epochs = 5\n",
    "batches_per_epoch = 500\n",
    "batch_size = 10\n",
    "lr = 1e-2\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print(\">>Generated: \", generate_sample(s, n_snippets=6))\n",
    "    print(\"-------\\n\")\n",
    "    avg_cost = 0\n",
    "    for batch in range(batches_per_epoch):\n",
    "        x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "        _, iloss, y_pred = s.run([train_op, loss, prediction], {X: x_,\n",
    "                                                                y: y_,\n",
    "                                                                lengths: len_,\n",
    "                                                                learning_rate_ph: lr})\n",
    "        avg_cost += iloss\n",
    "\n",
    "    print(\"EPOCH: \", epoch)\n",
    "    print(\"AVERAGE LOSS: \", avg_cost/batches_per_epoch)\n",
    "    print(\">>Predicted: \")\n",
    "    print_pred(y_pred)\n",
    "\n",
    "print(\">>Generated: \", generate_sample(s, n_snippets=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Putileen',\n",
       " 'Putisterne',\n",
       " 'Putian',\n",
       " 'Putiley',\n",
       " 'Puticat',\n",
       " 'Putine',\n",
       " 'Putilleyntt',\n",
       " 'Putint',\n",
       " 'Putil',\n",
       " 'Putiane',\n",
       " 'Putiley',\n",
       " 'Putilea']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Puti', n_snippets=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quilyn', 'Quricanne', 'Quertholina', 'Queby', 'Quera', 'Quudnia']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Q', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eugwellin', 'Eugry', 'Eugwon', 'Eugtia', 'Eugeldond', 'Eugwey']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Eug', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Luondem',\n",
       " 'Lustileh',\n",
       " 'Luzedie',\n",
       " 'Luendaina',\n",
       " 'Luscio',\n",
       " 'Luzta',\n",
       " 'Luaria',\n",
       " 'Luggy',\n",
       " 'Lury',\n",
       " 'Lueltpa',\n",
       " 'Luann',\n",
       " 'Ludill',\n",
       " 'Luuston',\n",
       " 'Luene',\n",
       " 'Lublynn',\n",
       " 'Luskelyna',\n",
       " 'Luelte',\n",
       " 'Lustadinsh',\n",
       " 'Lurislie']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Lu', n_snippets=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуйте использовать несколько слоев рекуррентных сетей\n",
    "\n",
    "* Попробуйте поменять код модели, встроив туда модуль как в примере в следующей ячейке;\n",
    "* Попробуйте использовать другие cells: LSTM, GRU;\n",
    "* Попробуй генерировать твиты, скачав [датасет](http://study.mokoron.com) или какой угодно другой датасет\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Как ненапряжно замутить глубокую рекуррентную нейросеть\n",
    "\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
