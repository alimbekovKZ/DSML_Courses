{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE-559: Practical Session 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this session is to illustrate on a 2D synthetic toy data-set how poorly a naive weight initialization procedure performs when a network has multiple layers of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function\n",
    "1. generate disc set(nb)\n",
    "\n",
    "that returns a pair torch.Tensor , torch.LongTensor of dimensions respectively nb * 2 and nb ,corresponding to the input and target of a toy data-set where the input is uniformly distributed in[-1; 1] * [-1; 1] and the label is 1 inside the disc of radius\n",
    "\n",
    "Create a train and test set of 1; 000 samples, and normalize their mean and variance to 0 and 1. A simple sanity check is to ensure that the two classes are balanced.\n",
    "\n",
    "**Hint**: My version of generate disc set is 172 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = Tensor(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions\n",
    "\n",
    "1. train model(model, train input, train target)\n",
    "1. compute nb errors(model, data input, data target)\n",
    "\n",
    "The first should train the model with cross-entropy and 250 epochs of standard sgd with nu = 0:1, and\n",
    "mini-batches of size 100.\n",
    "\n",
    "The second should also use mini-batches, and return an integer.\n",
    "\n",
    "**Hint**: My versions of train model and compute nb errors are respectively 512 and 457 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "    nb_epochs = 250\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write\n",
    "\n",
    "1. create shallow model()\n",
    "\n",
    "that returns a mlp with 2 input units, a single hidden layer of size 128, and 2 output units, and\n",
    "\n",
    "2. create deep model()\n",
    "\n",
    "that returns a mlp with 2 input units, hidden layers of sizes respectively 4; 8; 16; 32; 64; 128, and 2\n",
    "output units.\n",
    "\n",
    "**Hint**: You can use the nn.Sequential container to make things simpler. My versions of these two\n",
    "functions are respectively 132 and 355 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )\n",
    "\n",
    "def create_deep_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the train and test errors of these two models when they are initialized either with\n",
    "the default pytorch rule, or with a normal distribution of standard deviation 10^-3; 10^-2; 10^-1; 1; and\n",
    "10.\n",
    "\n",
    "The error rate with the shallow network for any initialization should be around 1:5%. It should be around 3% with the deep network using the default rule, and around 50% most of the time with the other initializations.\n",
    "\n",
    "**Hint**: My version is 562 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std create_shallow_model -1.000000 train_error 0.40% test_error 1.10%\n",
      "std create_deep_model -1.000000 train_error 3.60% test_error 3.40%\n",
      "std create_shallow_model 0.001000 train_error 1.60% test_error 2.00%\n",
      "std create_deep_model 0.001000 train_error 49.60% test_error 50.10%\n",
      "std create_shallow_model 0.010000 train_error 1.20% test_error 1.70%\n",
      "std create_deep_model 0.010000 train_error 49.60% test_error 50.10%\n",
      "std create_shallow_model 0.100000 train_error 0.80% test_error 1.90%\n",
      "std create_deep_model 0.100000 train_error 49.60% test_error 50.10%\n",
      "std create_shallow_model 1.000000 train_error 0.60% test_error 1.00%\n",
      "std create_deep_model 1.000000 train_error 50.40% test_error 49.90%\n",
      "std create_shallow_model 10.000000 train_error 0.00% test_error 1.40%\n",
      "std create_deep_model 10.000000 train_error 50.40% test_error 49.90%\n"
     ]
    }
   ],
   "source": [
    "for std in [ -1, 1e-3, 1e-2, 1e-1, 1e-0, 1e1 ]:\n",
    "\n",
    "    for m in [ create_shallow_model, create_deep_model ]:\n",
    "\n",
    "        model = m()\n",
    "\n",
    "        if std > 0:\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p.normal_(0, std)\n",
    "\n",
    "        train_model(model, train_input, train_target)\n",
    "\n",
    "        print('std {:s} {:f} train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "            m.__name__,\n",
    "            std,\n",
    "            compute_nb_errors(model, train_input, train_target) / train_input.size(0) * 100,\n",
    "            compute_nb_errors(model, test_input, test_target) / test_input.size(0) * 100\n",
    "        )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
