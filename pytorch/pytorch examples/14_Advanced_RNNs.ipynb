{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_Advanced_RNNs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "source": [
        "# Advanced RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this notebook we're going to cover some advanced topics related to RNNs.\n",
        "\n",
        "1. Conditioned hidden state\n",
        "2. Char-level embeddings\n",
        "3. Encoder and decoder\n",
        "4. Attentional mechanisms\n",
        "5. Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41r7MWJnY0m8",
        "colab_type": "text"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJDhjHCHY0_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "13e9f785-4995-4186-ae07-1c7971ca4f25"
      },
      "source": [
        "# Load PyTorch library\n",
        "!pip3 install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0FbOd6IZmzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOsqAo4XZpXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def create_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfvEzQ9ZweF",
        "colab_type": "code",
        "outputId": "1682f969-34d0-4856-9087-33d798de65cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=True,\n",
        "    batch_size=4,\n",
        "    condition_vocab_size=3, # vocabulary for condition possibilities\n",
        "    embedding_dim=100,\n",
        "    rnn_hidden_dim=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "source": [
        "# Conditioned RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUsj7HjBp69f",
        "colab_type": "text"
      },
      "source": [
        "Conditioning an RNN is to add extra information that will be helpful towards a prediction. We can encode (embed it) this information and feed it along with the sequential input into our model. For example, suppose in our document classificaiton example in the previous notebook, we knew the publisher of each news article (NYTimes, ESPN, etc.). We could have encoded that information to help with the prediction. There are several different ways of creating a conditioned RNN.\n",
        "\n",
        "**Note**: If the conditioning information is novel for each input in the sequence, just concatenate it along with each time step's input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc8H9JySmtLa",
        "colab_type": "text"
      },
      "source": [
        "1. Make the initial hidden state the encoded information instead of using the initial zerod hidden state. Make sure that the size of the encoded information is the same as the hidden state for the RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKlb9SjfpbED",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conditioned_rnn1.png\" width=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrlQHx2x8Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFoiV-fqmvRo",
        "colab_type": "code",
        "outputId": "87a48a1c-d012-4d2e-f299-7c80cc41b47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Condition\n",
        "condition = torch.LongTensor([0, 2, 1, 2]) # batch size of 4 with a vocab size of 3\n",
        "condition_embeddings = nn.Embedding(\n",
        "    embedding_dim=args.embedding_dim, # should be same as RNN hidden dim\n",
        "    num_embeddings=args.condition_vocab_size) # of unique conditions\n",
        "\n",
        "# Initialize hidden state\n",
        "num_directions = 1\n",
        "if args.bidirectional:\n",
        "    num_directions = 2\n",
        "    \n",
        "# If using multiple layers and directions, the hidden state needs to match that size\n",
        "hidden_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "    args.num_layers * num_directions, 1, 1).to(args.device) # initial state to RNN\n",
        "print (hidden_t.size())\n",
        "\n",
        "# Feed into RNN\n",
        "# y_out, _ = self.rnn(x_embedded, hidden_t)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgyaMDgmtHw",
        "colab_type": "text"
      },
      "source": [
        "2. Concatenate the encoded information with the hidden state at each time step. Do not replace the hidden state because the RNN needs that to learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUIg5o-dpiZF",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conditioned_rnn2.png\" width=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ-h28o-pi4X",
        "colab_type": "code",
        "outputId": "00e8f8ef-7cba-46e3-dd16-cbd15db33638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Initialize hidden state\n",
        "hidden_t = torch.zeros((args.num_layers * num_directions, args.batch_size, args.rnn_hidden_dim))\n",
        "print (hidden_t.size())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6hYSIdqBQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat_condition(condition_embeddings, condition, hidden_t, num_layers, num_directions):\n",
        "    condition_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "        num_layers * num_directions, 1, 1)\n",
        "    hidden_t = torch.cat([hidden_t, condition_t], 2)\n",
        "    return hidden_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjyzq_s5pixL",
        "colab_type": "code",
        "outputId": "4eb2f8c7-45f8-45be-da0c-2e05d517420c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Loop through the inputs time steps\n",
        "hiddens = []\n",
        "seq_size = 1\n",
        "for t in range(seq_size):\n",
        "    hidden_t = concat_condition(condition_embeddings, condition, hidden_t, \n",
        "                                args.num_layers, num_directions).to(args.device)\n",
        "    print (hidden_t.size())\n",
        "    \n",
        "    # Feed into RNN\n",
        "    # hidden_t = rnn_cell(x_in[t], hidden_t)\n",
        "    ..."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0_81jMXg_J",
        "colab_type": "text"
      },
      "source": [
        "# Char-level embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0yUKKpq3pu_",
        "colab_type": "text"
      },
      "source": [
        "Our conv operations will have inputs that are words in a sentence represented at the character level|  $\\in \\mathbb{R}^{NXSXWXE}$  and outputs are embeddings for each word (based on convlutions applied at the character level.) \n",
        "\n",
        "**Word embeddings**: capture the temporal correlations among\n",
        "adjacent tokens so that similar words have similar representations. Ex. \"New Jersey\" is close to \"NJ\" is close to \"Garden State\", etc.\n",
        "\n",
        "**Char embeddings**: create representations that map words at a character level. Ex. \"toy\" and \"toys\" will be close to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SZgVuwebm_4",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/char_embeddings.png\" width=450>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdIvz0G3O8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    vocab_size=20, # vocabulary\n",
        "    seq_size=10, # max length of each sentence\n",
        "    word_size=15, # max length of each word\n",
        "    embedding_dim=100,\n",
        "    num_filters=100, # filters per size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raztXIeYXYJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
        "                 num_output_channels, padding_idx):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        # Char-level embedding\n",
        "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                       num_embeddings=num_embeddings,\n",
        "                                       padding_idx=padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x.size()\n",
        "        batch_size, seq_len, word_len = input_shape\n",
        "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x = self.embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "        \n",
        "        # Convolution\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        \n",
        "        # Pooling\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
        "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z = torch.cat(z, 2) # join conv outputs\n",
        "        \n",
        "        return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzHVs8Xe0Zph",
        "colab_type": "code",
        "outputId": "0eb8429f-0c04-4da6-89c8-67c81c107986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Input\n",
        "input_size = (args.batch_size, args.seq_size, args.word_size)\n",
        "x_in = torch.randint(low=0, high=args.vocab_size, size=input_size).long()\n",
        "print (x_in.size())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_Xscby2PMQ",
        "colab_type": "code",
        "outputId": "fd70c783-03fd-4662-d76a-46e0a82a5f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# Initial char-level embedding model\n",
        "model = Model(embedding_dim=args.embedding_dim, \n",
        "              num_embeddings=args.vocab_size, \n",
        "              num_input_channels=args.embedding_dim, \n",
        "              num_output_channels=args.num_filters,\n",
        "              padding_idx=0)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of Model(\n",
            "  (embeddings): Embedding(20, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DIgeEZFXYR2",
        "colab_type": "code",
        "outputId": "9a465250-7fe8-4863-bf6e-d50ee15bb5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Forward pass to get char-level embeddings\n",
        "z = model(x_in)\n",
        "print (z.size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzTscaE10HFA",
        "colab_type": "text"
      },
      "source": [
        "There are several different ways you can use these char-level embeddings:\n",
        "\n",
        "1. Concat char-level embeddings with word-level embeddings, since we have an embedding for each word (at a char-level) and then feed it into an RNN. \n",
        "2. You can feed the char-level embeddings into an RNN to processes them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyCQ13_ckV_c",
        "colab_type": "text"
      },
      "source": [
        "# Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sixbu74kbJk",
        "colab_type": "text"
      },
      "source": [
        "So far we've used RNNs to `encode` a sequential input and generate hidden states. We use these hidden states to `decode` the predictions. So far, the encoder was an RNN and the decoder was just a few fully connected layers followed by a softmax layer (for classification). But the encoder and decoder can assume other architectures as well. For example, the decoder could be an RNN that processes the hidden state outputs from the encoder RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfK1mAp1dlpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    batch_size=64,\n",
        "    embedding_dim=100,\n",
        "    rnn_hidden_dim=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout=0.1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_OJFyY97bF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                 num_layers, bidirectional, padding_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_embeddings,\n",
        "                                            padding_idx=padding_idx)\n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
        "                          num_layers=num_layers, batch_first=True, \n",
        "                          bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x_in, x_lengths):\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_in)\n",
        "   \n",
        "        # Feed into RNN\n",
        "        out, h_n = self.gru(z)\n",
        "        \n",
        "        # Gather the last relevant hidden state\n",
        "        out = gather_last_relevant_hidden(out, x_lengths)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXtaGPlpyH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_output, apply_softmax=False):\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(encoder_output)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnKyCPVj-OVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                 hidden_dim, num_layers, bidirectional, output_dim, dropout_p, \n",
        "                 padding_idx=0):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                               num_layers, bidirectional, padding_idx=0)\n",
        "        self.decoder = Decoder(rnn_hidden_dim, hidden_dim, output_dim, dropout_p)\n",
        "        \n",
        "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_in, x_lengths)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeoErsc-Tum",
        "colab_type": "code",
        "outputId": "f8d295de-e06f-4e2e-bd25-02bb2339dbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "model = Model(embedding_dim=args.embedding_dim, num_embeddings=1000, \n",
        "              rnn_hidden_dim=args.rnn_hidden_dim, hidden_dim=args.hidden_dim, \n",
        "              num_layers=args.num_layers, bidirectional=args.bidirectional, \n",
        "              output_dim=4, dropout_p=args.dropout, padding_idx=0)\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of Model(\n",
            "  (encoder): Encoder(\n",
            "    (word_embeddings): Embedding(1000, 100, padding_idx=0)\n",
            "    (gru): GRU(100, 100, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (dropout): Dropout(p=0.1)\n",
            "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAsOI6jEmTd0",
        "colab_type": "text"
      },
      "source": [
        "# Attentional mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJN5ft5Sc_kb",
        "colab_type": "text"
      },
      "source": [
        "When processing an input sequence with an RNN, recall that at each time step we process the input and the hidden state at that time step. For many use cases, it's advantageous to have access to the inputs at all time steps and pay selective attention to the them at each time step. For example, in machine translation, it's advantageous to have access to all the words when translating to another language because translations aren't necessarily word for word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6A6WfbXje6",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/attention1.jpg\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNkayU0rf-ua",
        "colab_type": "text"
      },
      "source": [
        "Attention can sound a bit confusing so let's see what happens at each time step. At time step j, the model has processed inputs $x_0, x_1, x_2, ..., x_j$ and has generted hidden states $h_0, h_1, h_2, ..., h_j$. The idea is to use all the processed hidden states to make the prediction and not just the most recent one. There are several approaches to how we can do this.\n",
        "\n",
        "With **soft attention**, we learn a vector of floating points (probabilities) to multiply with the hidden states to create the context vector.\n",
        "\n",
        "Ex. [0.1, 0.3, 0.1, 0.4, 0.1]\n",
        "\n",
        "With **hard attention**, we can learn a binary vector to multiply with the hidden states to create the context vector. \n",
        "\n",
        "Ex. [0, 0, 0, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYSIAVQqu3Ab",
        "colab_type": "text"
      },
      "source": [
        "We're going to focus on soft attention because it's more widley used and we can visualize how much of each hidden state helps with the prediction, which is great for interpretability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ch21nZNvDHO",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/attention2.jpg\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_jPXuT8xlqd",
        "colab_type": "text"
      },
      "source": [
        "We're going to implement attention in the document classification task below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0iNnQzdxnGvn"
      },
      "source": [
        "# Document classification with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n38ZJoVZnGaE"
      },
      "source": [
        "We're going to implement the same document classification task as in the previous notebook but we're going to use an attentional interface for interpretability.\n",
        "\n",
        "**Why not machine translation?** Normally, machine translation is the go-to example for demonstrating attention but it's not really practical. How many situations can you think of that require a seq to generate another sequence? Instead we're going to apply attention with our document classification example to see which input tokens are more influential towards predicting the genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fu7HgEqbnGFY"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elL6BxtCmNGf",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCf2fLmPbKKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def create_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "19f4998e-3ac0-47ea-b8f0-0a90307db9bd",
        "id": "TTwkuoZdmMlF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=True,\n",
        "    shuffle=True,\n",
        "    data_file=\"news.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"news\",\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    pretrained_embeddings=None,\n",
        "    cutoff=25,\n",
        "    num_epochs=5,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=128,\n",
        "    embedding_dim=100,\n",
        "    kernels=[3,5],\n",
        "    num_filters=100,\n",
        "    rnn_hidden_dim=128,\n",
        "    hidden_dim=200,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout_p=0.25,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "create_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xfiWhgX5mMQ5"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baAsxXNFmMCF",
        "colab": {}
      },
      "source": [
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3tJi_HyOmLw-",
        "colab": {}
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4b1746c-82d9-42d8-ddc2-e4a5deb73c08",
        "id": "wrI_df4bmLjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category                                              title\n",
              "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
              "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
              "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
              "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
              "4  Business  Oil prices soar to all-time record, posing new..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "38d71042-5649-4b9c-ceea-2860cd409aef",
        "id": "TreK7nqEmLTN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "by_category = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_category[row.category].append(row.to_dict())\n",
        "for category in by_category:\n",
        "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Business: 30000\n",
            "Sci/Tech: 30000\n",
            "Sports: 30000\n",
            "World: 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35nb3LxLmLCA",
        "colab": {}
      },
      "source": [
        "final_list = []\n",
        "for _, item_list in sorted(by_category.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f153fa08-e867-4aca-a92e-3c56ce37ceb4",
        "id": "Y48IvuSfmK07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    84000\n",
              "val      18000\n",
              "test     18000\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWuNBxAXmKk2",
        "outputId": "2732826a-dca9-4fd7-8580-eb14fcbc8381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "    \n",
        "split_df.title = split_df.title.apply(preprocess_text)\n",
        "split_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>split</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>general electric posts higher rd quarter profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>lilly to eliminate up to us jobs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>s amp p lowers america west outlook to negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>does rand walk the talk on labor policy ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>housekeeper advocates for changes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category  split                                            title\n",
              "0  Business  train  general electric posts higher rd quarter profit\n",
              "1  Business  train                 lilly to eliminate up to us jobs\n",
              "2  Business  train  s amp p lowers america west outlook to negative\n",
              "3  Business  train        does rand walk the talk on labor policy ?\n",
              "4  Business  train                housekeeper advocates for changes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m-a0OpqhmKJc"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUMQ_MwumJ8F",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LtYf3lpExBb",
        "colab_type": "code",
        "outputId": "d2ee1d2d-fa55-4561-f911-b63a83cc6cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Vocabulary instance\n",
        "category_vocab = Vocabulary()\n",
        "for index, row in df.iterrows():\n",
        "    category_vocab.add_token(row.category)\n",
        "print (category_vocab) # __str__\n",
        "print (len(category_vocab)) # __len__\n",
        "index = category_vocab.lookup_token(\"Business\")\n",
        "print (index)\n",
        "print (category_vocab.lookup_index(index))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=4)>\n",
            "4\n",
            "0\n",
            "Business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0zkF6CsE_yH",
        "colab_type": "text"
      },
      "source": [
        "## Sequence vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtntaISyE_1c",
        "colab_type": "text"
      },
      "source": [
        "Next, we're going to create our Vocabulary classes for the article's title, which is a sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovI8QRefEw_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3ZouuTEw1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self.mask_token = mask_token\n",
        "        self.unk_token = unk_token\n",
        "        self.begin_seq_token = begin_seq_token\n",
        "        self.end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self.mask_token)\n",
        "        self.unk_index = self.add_token(self.unk_token)\n",
        "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
        "        \n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self.unk_token,\n",
        "                         'mask_token': self.mask_token,\n",
        "                         'begin_seq_token': self.begin_seq_token,\n",
        "                         'end_seq_token': self.end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx.get(token, self.unk_index)\n",
        "    \n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5UHjpi3El37",
        "colab_type": "code",
        "outputId": "a4e3a510-be1a-4f94-f3c6-69d5386e6916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Get word counts\n",
        "word_counts = Counter()\n",
        "for title in split_df.title:\n",
        "    for token in title.split(\" \"):\n",
        "        if token not in string.punctuation:\n",
        "            word_counts[token] += 1\n",
        "\n",
        "# Create SequenceVocabulary instance\n",
        "title_word_vocab = SequenceVocabulary()\n",
        "for word, word_count in word_counts.items():\n",
        "    if word_count >= args.cutoff:\n",
        "        title_word_vocab.add_token(word)\n",
        "print (title_word_vocab) # __str__\n",
        "print (len(title_word_vocab)) # __len__\n",
        "index = title_word_vocab.lookup_token(\"general\")\n",
        "print (index)\n",
        "print (title_word_vocab.lookup_index(index))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4400)>\n",
            "4400\n",
            "4\n",
            "general\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_wja0EfQNpA",
        "colab_type": "text"
      },
      "source": [
        "We're also going to create an instance fo SequenceVocabulary that processes the input on a character level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpfS0BXP9pz",
        "colab_type": "code",
        "outputId": "23ce4b1a-3006-40c4-cc1b-645506108e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Create SequenceVocabulary instance\n",
        "title_char_vocab = SequenceVocabulary()\n",
        "for title in split_df.title:\n",
        "    for token in title:\n",
        "        title_char_vocab.add_token(token)\n",
        "print (title_char_vocab) # __str__\n",
        "print (len(title_char_vocab)) # __len__\n",
        "index = title_char_vocab.lookup_token(\"g\")\n",
        "print (index)\n",
        "print (title_char_vocab.lookup_index(index))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=35)>\n",
            "35\n",
            "4\n",
            "g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dag6H0SFHAG",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQIfxcUuKwzz",
        "colab_type": "text"
      },
      "source": [
        "Something new that we introduce in this Vectorizer is calculating the length of our input sequence. We will use this later on to extract the last relevant hidden state for each input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsNtEnhBEl6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsVectorizer(object):\n",
        "    def __init__(self, title_word_vocab, title_char_vocab, category_vocab):\n",
        "        self.title_word_vocab = title_word_vocab\n",
        "        self.title_char_vocab = title_char_vocab\n",
        "        self.category_vocab = category_vocab\n",
        "\n",
        "    def vectorize(self, title):\n",
        "       \n",
        "        # Word-level vectorization\n",
        "        word_indices = [self.title_word_vocab.lookup_token(token) for token in title.split(\" \")]\n",
        "        word_indices = [self.title_word_vocab.begin_seq_index] + word_indices + \\\n",
        "            [self.title_word_vocab.end_seq_index]\n",
        "        title_length = len(word_indices)\n",
        "        word_vector = np.zeros(title_length, dtype=np.int64)\n",
        "        word_vector[:len(word_indices)] = word_indices\n",
        "        \n",
        "        # Char-level vectorization\n",
        "        word_length = max([len(word) for word in title.split(\" \")])\n",
        "        char_vector = np.zeros((len(word_vector), word_length), dtype=np.int64)\n",
        "        char_vector[0, :] = self.title_word_vocab.mask_index # <BEGIN>\n",
        "        char_vector[-1, :] = self.title_word_vocab.mask_index # <END>\n",
        "        for i, word in enumerate(title.split(\" \")):\n",
        "            char_vector[i+1,:len(word)] = [title_char_vocab.lookup_token(char) \\\n",
        "                                           for char in word] # i+1 b/c of <BEGIN> token\n",
        "                \n",
        "        return word_vector, char_vector, len(word_indices)\n",
        "    \n",
        "    def unvectorize_word_vector(self, word_vector):\n",
        "        tokens = [self.title_word_vocab.lookup_index(index) for index in word_vector]\n",
        "        title = \" \".join(token for token in tokens)\n",
        "        return title\n",
        "    \n",
        "    def unvectorize_char_vector(self, char_vector):\n",
        "        title = \"\"\n",
        "        for word_vector in char_vector:\n",
        "            for index in word_vector:\n",
        "                if index == self.title_char_vocab.mask_index:\n",
        "                    break\n",
        "                title += self.title_char_vocab.lookup_index(index)\n",
        "            title += \" \"\n",
        "        return title\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df, cutoff):\n",
        "        \n",
        "        # Create class vocab\n",
        "        category_vocab = Vocabulary()        \n",
        "        for category in sorted(set(df.category)):\n",
        "            category_vocab.add_token(category)\n",
        "\n",
        "        # Get word counts\n",
        "        word_counts = Counter()\n",
        "        for title in df.title:\n",
        "            for token in title.split(\" \"):\n",
        "                word_counts[token] += 1\n",
        "        \n",
        "        # Create title vocab (word level)\n",
        "        title_word_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                title_word_vocab.add_token(word)\n",
        "                \n",
        "        # Create title vocab (char level)\n",
        "        title_char_vocab = SequenceVocabulary()\n",
        "        for title in df.title:\n",
        "            for token in title:\n",
        "                title_char_vocab.add_token(token)\n",
        "        \n",
        "        return cls(title_word_vocab, title_char_vocab, category_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        title_word_vocab = SequenceVocabulary.from_serializable(contents['title_word_vocab'])\n",
        "        title_char_vocab = SequenceVocabulary.from_serializable(contents['title_char_vocab'])\n",
        "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
        "        return cls(title_word_vocab=title_word_vocab, \n",
        "                   title_char_vocab=title_char_vocab, \n",
        "                   category_vocab=category_vocab)\n",
        "    \n",
        "    def to_serializable(self):\n",
        "        return {'title_word_vocab': self.title_word_vocab.to_serializable(),\n",
        "                'title_char_vocab': self.title_char_vocab.to_serializable(),\n",
        "                'category_vocab': self.category_vocab.to_serializable()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtRRXU53El9Y",
        "colab_type": "code",
        "outputId": "b4801364-ebbd-4d5b-f0ad-7502aacebca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
        "print (vectorizer.title_word_vocab)\n",
        "print (vectorizer.title_char_vocab)\n",
        "print (vectorizer.category_vocab)\n",
        "word_vector, char_vector, title_length = vectorizer.vectorize(preprocess_text(\n",
        "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
        "print (\"word_vector:\", np.shape(word_vector))\n",
        "print (\"char_vector:\", np.shape(char_vector))\n",
        "print (\"title_length:\", title_length)\n",
        "print (word_vector)\n",
        "print (char_vector)\n",
        "print (vectorizer.unvectorize_word_vector(word_vector))\n",
        "print (vectorizer.unvectorize_char_vector(char_vector))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4404)>\n",
            "<SequenceVocabulary(size=35)>\n",
            "<Vocabulary(size=4)>\n",
            "word_vector: (10,)\n",
            "char_vector: (10, 10)\n",
            "title_length: 10\n",
            "[   2    1 4151 1231   25    1 2392 4076   38    3]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7 15  4  5  7  0  0  0  0  0]\n",
            " [21  5 18  5  7  5  7  0  0  0]\n",
            " [26 13  6 16  0  0  0  0  0  0]\n",
            " [12 17  5  0  0  0  0  0  0  0]\n",
            " [26 13 23 25  9  5 18 15  6  0]\n",
            " [12  5  6  6 13 16  0  0  0  0]\n",
            " [12 15 20  7  6  8 23  5  6 12]\n",
            " [30  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0]]\n",
            "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n",
            " roger federer wins the wimbledon tennis tournament .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk_QvpVfFM0S",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU7oDdelFMR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7FHmiSFMXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.category.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, df, cutoff):\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, df, vectorizer_filepath):\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NewsVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        title_word_vector, title_char_vector, title_length = \\\n",
        "            self.vectorizer.vectorize(row.title)\n",
        "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
        "        return {'title_word_vector': title_word_vector, \n",
        "                'title_char_vector': title_char_vector, \n",
        "                'title_length': title_length, \n",
        "                'category': category_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
        "                         drop_last=False, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
        "                                collate_fn=collate_fn, shuffle=shuffle, \n",
        "                                drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dpb6ZHJFMeb",
        "colab_type": "code",
        "outputId": "7e45c3e8-dfd1-49f0-c0cf-9401e334249f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# Dataset instance\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df,\n",
        "                                                       cutoff=args.cutoff)\n",
        "print (dataset) # __str__\n",
        "input_ = dataset[10] # __getitem__\n",
        "print (input_['title_word_vector'])\n",
        "print (input_['title_char_vector'])\n",
        "print (input_['title_length'])\n",
        "print (input_['category'])\n",
        "print (dataset.vectorizer.unvectorize_word_vector(input_['title_word_vector']))\n",
        "print (dataset.vectorizer.unvectorize_char_vector(input_['title_char_vector']))\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=84000)\n",
            "[ 2 51  1 52 53 26 54  3]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [18  5  9 12  8  0  0  0  0  0]\n",
            " [18 15 18  4  5 16  0  0  0  0]\n",
            " [25  8  6 27  7 20 14 12 11 22]\n",
            " [26 13 12 17  0  0  0  0  0  0]\n",
            " [ 9  8 25 15  7  0  0  0  0  0]\n",
            " [18  5  8  9  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0]]\n",
            "8\n",
            "0\n",
            "<BEGIN> delta <UNK> bankruptcy with labor deal <END>\n",
            " delta dodges bankruptcy with labor deal  \n",
            "tensor([3.3333e-05, 3.3333e-05, 3.3333e-05, 3.3333e-05])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUIqtbvFUAG",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJV5WlDiFVVz",
        "colab_type": "text"
      },
      "source": [
        "embed  encoder  attend  predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZCzdZZ9FMhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9wipRZt7feC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
        "                 kernels, num_input_channels, num_output_channels, \n",
        "                 rnn_hidden_dim, num_layers, bidirectional, \n",
        "                 word_padding_idx=0, char_padding_idx=0):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_word_embeddings,\n",
        "                                            padding_idx=word_padding_idx)\n",
        "        self.char_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_char_embeddings,\n",
        "                                            padding_idx=char_padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, \n",
        "                                             num_output_channels, \n",
        "                                             kernel_size=f) for f in kernels])\n",
        "        \n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim*(len(kernels)+1), \n",
        "                          hidden_size=rnn_hidden_dim, num_layers=num_layers, \n",
        "                          batch_first=True, bidirectional=bidirectional)\n",
        "        \n",
        "    def initialize_hidden_state(self, batch_size, rnn_hidden_dim, device):\n",
        "        \"\"\"Modify this to condition the RNN.\"\"\"\n",
        "        num_directions = 1\n",
        "        if self.bidirectional:\n",
        "            num_directions = 2\n",
        "        hidden_t = torch.zeros(self.num_layers * num_directions, \n",
        "                               batch_size, rnn_hidden_dim).to(device)\n",
        "        \n",
        "    def get_char_level_embeddings(self, x):\n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x.size()\n",
        "        batch_size, seq_len, word_len = input_shape\n",
        "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x = self.char_embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # Convolution\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        \n",
        "        # Pooling\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
        "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z = torch.cat(z, 2) # join conv outputs\n",
        "        \n",
        "        return z\n",
        "        \n",
        "    def forward(self, x_word, x_char, x_lengths, device):\n",
        "        \"\"\"\n",
        "        x_word: word level representation (N, seq_size)\n",
        "        x_char: char level representation (N, seq_size, word_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_word)\n",
        "        \n",
        "        # Char level embeddings\n",
        "        z_char = self.get_char_level_embeddings(x=x_char)\n",
        "        \n",
        "        # Concatenate\n",
        "        z = torch.cat([z_word, z_char], 2)\n",
        "        \n",
        "        # Feed into RNN\n",
        "        initial_h = self.initialize_hidden_state(\n",
        "            batch_size=z.size(0), rnn_hidden_dim=self.gru.hidden_size,\n",
        "            device=device)\n",
        "        out, h_n = self.gru(z, initial_h)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeEcdA287gz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsDecoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
        "        super(NewsDecoder, self).__init__()\n",
        "        \n",
        "        # Attention FC layer\n",
        "        self.fc_attn = nn.Linear(rnn_hidden_dim, rnn_hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(rnn_hidden_dim))\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, apply_softmax=False):\n",
        "        \n",
        "        # Attention\n",
        "        z = torch.tanh(self.fc_attn(encoder_outputs))\n",
        "        z = z.transpose(2,1) # [B*H*T]\n",
        "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1) #[B*1*H]\n",
        "        z = torch.bmm(v,z).squeeze(1) # [B*T]\n",
        "        attn_scores = F.softmax(z, dim=1)\n",
        "        context = torch.matmul(encoder_outputs.transpose(-2, -1), \n",
        "                               attn_scores.unsqueeze(dim=2)).squeeze()\n",
        "        if len(context.size()) == 1:\n",
        "            context = context.unsqueeze(0)\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(context)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return attn_scores, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVDftS-G7gwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
        "                 kernels, num_input_channels, num_output_channels, \n",
        "                 rnn_hidden_dim, hidden_dim, output_dim, num_layers, \n",
        "                 bidirectional, dropout_p, word_padding_idx, char_padding_idx):\n",
        "        super(NewsModel, self).__init__()\n",
        "        self.encoder = NewsEncoder(embedding_dim, num_word_embeddings,\n",
        "                                   num_char_embeddings, kernels, \n",
        "                                   num_input_channels, num_output_channels, \n",
        "                                   rnn_hidden_dim, num_layers, bidirectional, \n",
        "                                   word_padding_idx, char_padding_idx)\n",
        "        self.decoder = NewsDecoder(rnn_hidden_dim, hidden_dim, output_dim, \n",
        "                                   dropout_p)\n",
        "        \n",
        "    def forward(self, x_word, x_char, x_lengths, device, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_word, x_char, x_lengths, device)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPYCPd7Fl3M",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3seBMA7FlcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnRKWLekFlnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, \n",
        "                 shuffle, num_epochs, batch_size, learning_rate, \n",
        "                 early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.device = device\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'done_training': False,\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "    \n",
        "    def pad_word_seq(self, seq, length):\n",
        "        vector = np.zeros(length, dtype=np.int64)\n",
        "        vector[:len(seq)] = seq\n",
        "        vector[len(seq):] = self.dataset.vectorizer.title_word_vocab.mask_index\n",
        "        return vector\n",
        "    \n",
        "    def pad_char_seq(self, seq, seq_length, word_length):\n",
        "        vector = np.zeros((seq_length, word_length), dtype=np.int64)\n",
        "        vector.fill(self.dataset.vectorizer.title_char_vocab.mask_index)\n",
        "        for i in range(len(seq)):\n",
        "            char_padding = np.zeros(word_length-len(seq[i]), dtype=np.int64)\n",
        "            vector[i] = np.concatenate((seq[i], char_padding), axis=None)\n",
        "        return vector\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "        \n",
        "        # Make a deep copy\n",
        "        batch_copy = copy.deepcopy(batch)\n",
        "        processed_batch = {\"title_word_vector\": [], \"title_char_vector\": [], \n",
        "                           \"title_length\": [], \"category\": []}\n",
        "             \n",
        "        # Max lengths\n",
        "        get_seq_length = lambda sample: len(sample[\"title_word_vector\"])\n",
        "        get_word_length = lambda sample: len(sample[\"title_char_vector\"][0])\n",
        "        max_seq_length = max(map(get_seq_length, batch))\n",
        "        max_word_length = max(map(get_word_length, batch))\n",
        "\n",
        "\n",
        "        # Pad\n",
        "        for i, sample in enumerate(batch_copy):\n",
        "            padded_word_seq = self.pad_word_seq(\n",
        "                sample[\"title_word_vector\"], max_seq_length)\n",
        "            padded_char_seq = self.pad_char_seq(\n",
        "                sample[\"title_char_vector\"], max_seq_length, max_word_length)\n",
        "            processed_batch[\"title_word_vector\"].append(padded_word_seq)\n",
        "            processed_batch[\"title_char_vector\"].append(padded_char_seq)\n",
        "            processed_batch[\"title_length\"].append(sample[\"title_length\"])\n",
        "            processed_batch[\"category\"].append(sample[\"category\"])\n",
        "            \n",
        "        # Convert to appropriate tensor types\n",
        "        processed_batch[\"title_word_vector\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_word_vector\"])\n",
        "        processed_batch[\"title_char_vector\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_char_vector\"])\n",
        "        processed_batch[\"title_length\"] = torch.LongTensor(\n",
        "            processed_batch[\"title_length\"])\n",
        "        processed_batch[\"category\"] = torch.LongTensor(\n",
        "            processed_batch[\"category\"])\n",
        "        \n",
        "        return processed_batch  \n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "                shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                # compute the output\n",
        "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                       x_char=batch_dict['title_char_vector'],\n",
        "                                       x_lengths=batch_dict['title_length'],\n",
        "                                       device=self.device)\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute gradients using loss\n",
        "                loss.backward()\n",
        "\n",
        "                # use optimizer to take a gradient step\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # initialize batch generator, set loss and acc to 0, set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "                shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                       x_char=batch_dict['title_char_vector'],\n",
        "                                       x_lengths=batch_dict['title_length'],\n",
        "                                       device=self.device)\n",
        "\n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        # initialize batch generator, set loss and acc to 0, set eval mode on\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
        "            shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
        "                                   x_char=batch_dict['title_char_vector'],\n",
        "                                   x_lengths=batch_dict['title_length'],\n",
        "                                   device=self.device)\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        self.train_state[\"done_training\"] = True\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICkiOaGtFlk-",
        "colab_type": "code",
        "outputId": "a9aaa735-802c-483e-b148-2bc5855c98a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# Initialization\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(df=split_df,\n",
        "                                                       cutoff=args.cutoff)\n",
        "dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  kernels=args.kernels,\n",
        "                  num_input_channels=args.embedding_dim,\n",
        "                  num_output_channels=args.num_filters,\n",
        "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "    )\n",
            "    (gru): GRU(300, 128, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.25)\n",
            "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
            "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuaRZ4DiFlh1",
        "colab_type": "code",
        "outputId": "65399f8d-2548-4935-dab3-1fc6af331b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 69.7% | [VAL LOSS]: 0.55 | [VAL ACC]: 80.5%\n",
            "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 0.50 | [TRAIN ACC]: 82.0% | [VAL LOSS]: 0.48 | [VAL ACC]: 82.4%\n",
            "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 0.44 | [TRAIN ACC]: 84.4% | [VAL LOSS]: 0.45 | [VAL ACC]: 83.7%\n",
            "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 0.39 | [TRAIN ACC]: 86.0% | [VAL LOSS]: 0.47 | [VAL ACC]: 83.2%\n",
            "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 0.36 | [TRAIN ACC]: 87.3% | [VAL LOSS]: 0.44 | [VAL ACC]: 84.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzRJIz88Flfe",
        "colab_type": "code",
        "outputId": "f7c6b59d-0177-49d1-f635-67275429d5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAE/CAYAAADVKysfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EmFhiX-FMaV",
        "colab_type": "code",
        "outputId": "191ce455-b6f1-4c1b-b874-85ab76b489d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.43\n",
            "Test Accuracy: 84.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVU1zakYFMVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLoKfjSpFw7t",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_PWM3xgXHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer, device=\"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.vectorizer = vectorizer\n",
        "        self.device = device\n",
        "  \n",
        "    def predict_category(self, dataset):\n",
        "        # Batch generator\n",
        "        batch_generator = dataset.generate_batches(\n",
        "            batch_size=len(dataset), shuffle=False, device=self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            attn_scores, y_pred =  self.model(\n",
        "                x_word=batch_dict['title_word_vector'],\n",
        "                x_char=batch_dict['title_char_vector'],\n",
        "                x_lengths=batch_dict['title_length'],\n",
        "                device=self.device,\n",
        "                apply_softmax=True)\n",
        "\n",
        "            # Top k nationalities\n",
        "            y_prob, indices = torch.topk(y_pred, k=len(self.vectorizer.category_vocab))\n",
        "            probabilities = y_prob.detach().to('cpu').numpy()[0]\n",
        "            indices = indices.detach().to('cpu').numpy()[0]\n",
        "\n",
        "            results = []\n",
        "            for probability, index in zip(probabilities, indices):\n",
        "                category = self.vectorizer.category_vocab.lookup_index(index)\n",
        "                results.append({'category': category, \n",
        "                                'probability': probability})\n",
        "\n",
        "        return attn_scores, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8UoSJPggXC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load vectorizer\n",
        "with open(args.vectorizer_file) as fp:\n",
        "    vectorizer = NewsVectorizer.from_serializable(json.load(fp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woxuiUA2gXAX",
        "colab_type": "code",
        "outputId": "6aeddf12-c0eb-4d7c-a060-608c146c397d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# Load the model\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  kernels=args.kernels,\n",
        "                  num_input_channels=args.embedding_dim,\n",
        "                  num_output_channels=args.num_filters,\n",
        "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "print (model.named_modules)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "    )\n",
            "    (gru): GRU(300, 128, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.25)\n",
            "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
            "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfc666nngW9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize\n",
        "inference = Inference(model=model, vectorizer=vectorizer, device=\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1soPBCApgW7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "        self.target_size = len(self.df)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(size={1})>\".format(self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        title_word_vector, title_char_vector, title_length = \\\n",
        "            self.vectorizer.vectorize(row.title)\n",
        "        return {'title_word_vector': title_word_vector, \n",
        "                'title_char_vector': title_char_vector, \n",
        "                'title_length': title_length}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=False, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU70BuYvgW4Q",
        "colab_type": "code",
        "outputId": "07ded3cf-0ffe-439c-81d7-488b105fbaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "# Inference\n",
        "title = input(\"Enter a title to classify: \")\n",
        "infer_df = pd.DataFrame([title], columns=['title'])\n",
        "infer_df.title = infer_df.title.apply(preprocess_text)\n",
        "infer_dataset = InferenceDataset(infer_df, vectorizer)\n",
        "attn_scores, results = inference.predict_category(dataset=infer_dataset)\n",
        "results"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a title to classify: test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e1d3dd173c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minfer_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minfer_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfer_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-8d402476279e>\u001b[0m in \u001b[0;36mpredict_category\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 apply_softmax=True)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Top k nationalities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-90d55101be4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_word, x_char, x_lengths, device, apply_softmax)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-3a41285838e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_word, x_char, x_lengths, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Char level embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mz_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_char_level_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Concatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-3a41285838e1>\u001b[0m in \u001b[0;36mget_char_level_embeddings\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-3a41285838e1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    195\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 196\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (4). Kernel size: (5). Kernel size can't be greater than actual input size"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3jrZ6ZkxN4r",
        "colab_type": "text"
      },
      "source": [
        "# Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrAieHoHxOt2",
        "colab_type": "text"
      },
      "source": [
        "We can inspect the probability vector that is generated at each time step to visualize the importance of each of the previous hidden states towards a particular time step's prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6uZY4J8vYgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQwfUfwzjE_F",
        "colab_type": "code",
        "outputId": "9c04fbfc-06b9-4ad0-9406-e725e653de95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "attn_matrix = attn_scores.detach().numpy()\n",
        "ax = sns.heatmap(attn_matrix, linewidths=2, square=True)\n",
        "tokens = [\"<BEGIN>\"]+preprocess_text(title).split(\" \")+[\"<END>\"]\n",
        "ax.set_xticklabels(tokens, rotation=45)\n",
        "ax.set_xlabel(\"Token\")\n",
        "ax.set_ylabel(\"Importance\\n\")\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3d1d07190acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattn_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<BEGIN>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<END>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attn_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CWwxX2aTBla",
        "colab_type": "text"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPzmk6T3TD_9",
        "colab_type": "text"
      },
      "source": [
        "Recall from our [CNN notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/11_Convolutional_Neural_Networks.ipynb) that we used batch normalization to deal with internal covariant shift. Our activations will experience the same issues with RNNs but we will use a technique known as [layer normalization](https://arxiv.org/abs/1607.06450) (layernorm) to maintain zero mean unit variance on the activations. \n",
        "\n",
        "With layernorm it's a bit different from batchnorm. We compute the mean and var for every single sample (instead of each hidden dim) for each layer independentlyand then do theoperations on the activations before they go through the nonlinearities. PyTorch's [LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) class abstracts all of this for us when we feed in inputs to the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTdk6OyTGq5",
        "colab_type": "text"
      },
      "source": [
        "$ LN = \\frac{a - \\mu_{L}}{\\sqrt{\\sigma^2_{L} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $ \\mu_{L}$ = mean of input| $\\in \\mathbb{R}^{NX1}$\n",
        "* $\\sigma^2_{L}$ = variance of input | $\\in \\mathbb{R}^{NX1}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZGISTXTGnV",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/layernorm.png\" width=400>\n",
        "\n",
        "The most useful location to apply layernorm will be inside the RNN on the activations before the non-linearities. However, this is a bit involved and though PyTorch has a [LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) class, they do not have an RNN that has built in layernorm yet. You could implement the RNN yourself and manually add layernorm by following a similar setup like below.\n",
        "\n",
        "```python\n",
        "# Layernorm\n",
        "for t in range(seq_size):\n",
        "    # Normalize over hidden dim\n",
        "    layernorm = nn.LayerNorm(args.hidden_dim)\n",
        "    # Activating the module\n",
        "    a = layernorm(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YHneO3SStOp",
        "colab_type": "text"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGHaKTe1SuEk",
        "colab_type": "text"
      },
      "source": [
        "- attn visualization isn't always great\n",
        "- bleu score\n",
        "- ngram-overlap\n",
        "- perplexity\n",
        "- beamsearch\n",
        "- hierarchical softmax\n",
        "- hierarchical attention\n",
        "- Transformer networks\n",
        "- attention interpretability is hit/miss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfecmaOv0VAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}